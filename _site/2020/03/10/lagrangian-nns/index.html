<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lagrangian Neural Networks</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="A science blog by Sam Greydanus">
    <link rel="canonical" href="http://greydanus.github.io/2020/03/10/lagrangian-nns/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Natural Intelligence posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/assets/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-72311215-6', 'auto');
      ga('send', 'pageview');

    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    
    <!-- Allow RSS subscribers, as described in https://dzhavat.github.io/2020/01/19/adding-an-rss-feed-to-github-pages.html -->
    <link rel="alternate" type="application/atom+xml" title="Sam Greydanus’ blog" href="/feed.xml">

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:11px; margin-right:10px;">
    <img src="/assets/oak.png" width="50px">
    </div>

    <div style="float:left; margin-top:4px; margin-right:10px;">
      <a class="site-title" style="padding-top:0px; padding-bottom:0px;" href="/ ">Natural Intelligence</a>
      <!--<p style="font-size:14px;">‎‎ㅤㅤㅤ&zwnj;</p> This is very hacky but I couldn't find a better way -->
      <br>
      <p style="padding-bottom:6px; font-size:15px;font-weight: 300;">A science blog by Sam Greydanus</p>
    </div>

<!--     <a class="site-title" style="padding-top:8px; font-size:28px" href="/ ">Natural Intelligence</a> -->
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger" style="margin-bottom:0px; padding-top:10px; font-size:13px;font-weight: 300;">
<!--         <a class="page-link" href="/ ">Home</a> -->
<!--         
          <a class="page-link" href="/assets/2017-12-23-neurips17/">A Review of NeurIPS</a>
        
          <a class="page-link" href="/2022-04-08-sparsity/">The Story of Sparsity in Neural Networks</a>
        
          <a class="page-link" href="/2022-04-12-studying-growth/">Studying Growth with Cellular Automata</a>
        
          <a class="page-link" href="/2022-04-26-structural-optimization/">A Tutorial on Structural Optimization</a>
        
          
        
          <a class="page-link" href="/idea-of-god/">The Idea of God</a>
        
          
        
          <a class="page-link" href="/justification/">Trimmer on Justification</a>
        
          <a class="page-link" href="/papers/">Interesting papers</a>
        
          
         -->
        <a class="page-link" style="margin-bottom: 0px; padding-bottom:0px;" href="../../../../index.html">Home</a>
<!--         <a class="page-link" style="margin-bottom: 0px; padding-bottom:0px;" href="../../../../about.html">About me (old)</a> -->
        <a class="page-link" style="margin-bottom: 0px; padding-bottom:0px;" href="/about_me">About me</a>
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Lagrangian Neural Networks</h1>
    <p class="meta">Mar 10, 2020 • With Miles Cranmer and Stephan Hoyer</p>
  </header>

  <article class="post-content">
  <p>Accurate models of the world are built on notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. But neural network models struggle to learn these symmetries. To address this shortcoming, last year I introduced a class of models called Hamiltonian Neural Networks (HNNs) that can learn these invariant quantities directly from (pixel) data. In this project, some friends and I are going to introduce a complimentary class of models called Lagrangian Neural Networks (LNNs). These models are able to learn Lagrangian functions straight from data. They’re interesting because, like HNNs, they can learn exact conservation laws, but unlike HNNs they don’t require canonical coordinates.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:90%">
	<img src="/assets/lagrangian-nns/overall-idea.png" style="padding: 0px 0px 10px 0px;" />
	<div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:90%"><b>Figure 1:</b> A Lagrangian Neural Network learns the Lagrangian of a double pendulum. In this post, we introduce Lagrangian Neural Networks (LNNs). Like Hamiltonian Neural Networks, they can learn arbitrary conservation laws. In some cases they are better since they do not require canonical coordinates.</div>
</div>

<div style="display: block; margin-left: auto; margin-right: auto; width:100%; text-align:center;">
	<a href="https://arxiv.org/abs/2003.04630" id="linkbutton" target="_blank">Read the paper</a>
	<a href="https://colab.research.google.com/drive/1CSy-xfrnTX28p1difoTA8ulYw0zytJkq" id="linkbutton" target="_blank"><span class="colab-span">Run</span> in browser</a>
	<a href="https://github.com/MilesCranmer/lagrangian_nns" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="a-scientific-poem">“A scientific poem”</h2>

<p><a href="https://en.wikipedia.org/wiki/Joseph-Louis_Lagrange">Joseph-Louis</a> <a href="https://www.famousscientists.org/joseph-louis-lagrange/">Lagrange</a> must have known that life is short. He was born to a family of 11 children and only two of them survived to adulthood. Then he spent his adult years in Paris, living through the Reign of Terror and <a href="https://books.google.com/books?id=_q7zCAAAQBAJ&amp;pg=PR28&amp;lpg=PR28&amp;dq=It+took+only+a+moment+to+cause+this+head+to+fall+and+a+hundred+years+will+not+suffice+to+produce+its+like.&amp;source=bl&amp;ots=pP-iyGhBRq&amp;sig=ACfU3U1CqtjR-wSD1Zlt3uZX6SEbUwNRqg&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwji8pma_pDoAhXXqZ4KHUl0DZcQ6AEwAHoECAgQAQ#v=onepage&amp;q=It%20took%20only%20a%20moment%20to%20cause%20this%20head%20to%20fall%20and%20a%20hundred%20years%20will%20not%20suffice%20to%20produce%20its%20like.&amp;f=false">losing some of his closest friends to the guillotine</a>. Sometimes I wonder if these hardships made him more sensitive to the world’s ephemeral beauty, and more determined to make the most of his short time here.</p>

<p>Indeed, his path into research was notable for its passion and suddenness. Until the age of 17, Lagrange was a normal youth who planned to become a lawyer and showed no particular interest in mathematics. But all of that changed when he read an inspiring memoir by Edmund Halley and decided to embark on an obsessive course of self-study in mathematics. A mere two years later he published the principle of least action.</p>

<blockquote>
  <p>“I will deduce the complete mechanics of solid and fluid bodies using the principle of least action.” – Joseph-Louis Lagrange, age 20</p>
</blockquote>

<div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:50%">
	<img src="/assets/lagrangian-nns/lagrange.png" />
	<div class="thecap">A French stamp commemorating Lagrange.</div>
</div>

<p>Lagrange’s work was notable for its purity and beauty, especially in contrast to the chaotic and broken times that he lived through. Expressing admiration for the principle of least action, William Hamilton once called it <a href="https://books.google.com/books?id=j_kJCAAAQBAJ&amp;pg=PA130&amp;lpg=PA130&amp;dq=joseph+lagrange+beauty+of+virtual+work&amp;source=bl&amp;ots=771naVFjo6&amp;sig=ACfU3U0L4Bj9IabO1jFh7jJK0pEgoVVfWg&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjAppGZtY7oAhXcGTQIHfp3CncQ6AEwAHoECAwQAQ#v=onepage&amp;q=joseph%20lagrange%20beauty%20of%20virtual%20work&amp;f=false">“a scientific poem”</a>. In the following sections, I’ll introduce you to this “scientific poem” and then use it to derive Lagrangian Neural Networks.</p>

<h2 id="the-principle-of-least-action">The Principle of Least Action</h2>

<p><strong>The Action.</strong> Start with any physical system that has coordinates \(x_t = (q, \dot q)\). For example, we might describe a double pendulum using the angles of its arms and their respective angular velocities. Now, one simple observation is that these coordinates must start in one state \(x_0\) and end up in another, \(x_1\). There are many paths that these coordinates might take as they pass from \(x_0\) to \(x_1\), and we can associate each of these paths with a scalar value \(S\) called “the action.” Lagrangian mechanics tells us that the action is related to kinetic and potential energy, \(T\) and \(V\), by a functional</p>

<p>\(\begin{equation}
S ~=~ \int_{t_0}^{t_1} T(q_t, \dot q_t) - V(q_t, \dot q_t) ~~ dt.
\label{eq:eqn1}
\tag{1}
\end{equation}\)
<!-- <div class="thecap" style="text-align:center; width:50%"><b>Figure 3:</b> Possible paths from x0 to x1, plotted in configuration space. The action is stationary (δS = 0) for small perturbations (δq) to the path that the system actually takes (red). .</div> --></p>

<p>At first glance, \(S\) seems like an arbitrary combination of energies. But it has one remarkable property. It turns out that for all possible paths between \(x_0\) and \(x_1\), there is only one path that gives a stationary value of \(S\). Moreover, that path is the one that nature always takes.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:50%">
	<img src="/assets/lagrangian-nns/paths.png" style="width:100%" />
	<div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto;"><b><a href="https://en.wikipedia.org/wiki/Lagrangian_mechanics#/media/File:Least_action_principle.svg" target="_blank">Figure 3:</a></b> Possible paths from q0 to q1, plotted in <a href="https://en.wikipedia.org/wiki/Configuration_space_(physics)">configuration space</a>. The action is stationary (δS = 0) for small perturbations (δq) to the path that the system actually takes (red).</div>
</div>

<p><strong>The Euler-Lagrange equation.</strong> In order to “deduce the complete mechanics of solid and fluid bodies,” all Lagrange had to do was constrain every path to be a stationary point in \(S\). The modern principle of least action looks very similar: we let \(\mathcal{L} \equiv T - V\) (this is called the Lagrangian), and then write the constraint as \( \frac{d}{dt} \frac{\partial \mathcal{L}}{\partial \dot q_j} = \frac{\partial \mathcal{L}}{\partial q_j}\). Physicists call this constraint equation the <em>Euler-Lagrange equation</em>.</p>

<p>When you first encounter it, the principle of least action can seem abstract and impractical. But it can be quite easy to apply in practice. Consider, for example, a single particle with mass \(m\), position \(q\), and potential energy \(V(q)\):</p>

<p><span id="longEqnWithSmallScript_A" style="display:block; margin-left:auto;margin-right:auto;text-align:center;">
\(\begin{align}
\scriptstyle \mathcal{L} &amp; \scriptstyle ~=~ -V(q) + \frac{1}{2} m \dot q ^2 &amp; \scriptstyle \text{write down the Lagrangian} \quad (2)\\
\scriptstyle -\frac{\partial V(q)}{\partial q} &amp; \scriptstyle ~=~ m \ddot q &amp; \scriptstyle \text{apply Euler-Lagrange} \quad (3)\\
\scriptstyle F &amp; \scriptstyle ~=~ ma &amp; \scriptstyle \text{this is Newton's second law } \quad (4)\\
\end{align}\)
</span>
<span id="longEqnWithLargeScript_A" style="display:block; margin-left:auto;margin-right:auto;text-align:center;">
\(\begin{align}
\mathcal{L} &amp; ~=~ -V(q) + \frac{1}{2} m \dot q ^2 &amp; \text{write down the Lagrangian} \quad (2)\\
-\frac{\partial V(q)}{\partial q} &amp; ~=~ m \ddot q &amp;  \text{apply the Euler-Lagrange equation to } \mathcal{L} \quad (3)\\
 F &amp; ~=~ ma &amp; \text{this is Newton's second law } \quad (4)\\
\end{align}\)
</span></p>

<p><strong>Nature’s cost function.</strong> As a physicist who now does machine learning, I can’t help but think of \(S\) as Nature’s cost function. After all, it is a scalar quantity for which Nature finds a stationary point, usually a minimum, in order to generate the dynamics of the entire universe. The analogy gets even more interesting at small spatial scales, where quantum wavefunctions can be interpreted as Nature’s way of exploring multiple paths that are all very close to the path of stationary action.<sup id="fnref:fn1" role="doc-noteref"><a href="#fn:fn1" class="footnote">1</a></sup></p>

<h2 id="how-we-usually-solve-lagrangians">How we usually solve Lagrangians</h2>

<p>Ever since Lagrange introduced the notion of stationary action, physicists have followed a simple formula:</p>
<ol>
  <li>Find analytic expressions for kinetic and potential energy</li>
  <li>Write down the Lagrangian</li>
  <li>Apply the Euler-Lagrange constraint</li>
  <li>Solve the resulting system of differential equations</li>
</ol>

<p>But these analytic solutions are rather crude approximations of the real world. An alternative approach is to assume that the Lagrangian is an arbitrarily complicated function – a black box that does not permit analytical solutions. When this is the case, we must give up all hope of writing the Lagrangian out by hand. However, there is still a chance that we can parameterize it with a neural network and learn it straight from data. That is the main contribution of our recent paper.</p>

<h2 id="how-to-learn-lagrangians">How to learn Lagrangians</h2>

<p>The process of learning a Lagrangian differs from the traditional approach, but it also involves four basic steps:</p>
<ol>
  <li>Obtain data from a physical system</li>
  <li>Parameterize the Lagrangian with a neural network (\(\mathcal{L}\equiv \mathcal{L}_{\theta}\)).</li>
  <li>Apply the Euler-Lagrange constraint</li>
  <li>Backpropagate through the constraint to train a parametric model that approximates the true Lagrangian</li>
</ol>

<p>The first two steps are fairly straightforward, and we’ll see that automatic differentiation makes the fourth pretty painless. So let’s focus on step 3: applying the Euler-Lagrange constraint. Our angle of attack will be to write down the constraint equation, treat \(\mathcal{L}\) as a differentiable blackbox function, and see whether we can still obtain dynamics:</p>

<p><span id="longEqnWithSmallScript_B" style="display:block; margin-left:auto;margin-right:auto;text-align:center;">
\(\begin{align}
&amp; \scriptstyle \frac{d}{dt} \frac{\partial \mathcal{L}}{\partial \dot q_j} \scriptstyle ~=~ \frac{\partial \mathcal{L}}{\partial q_j} &amp; \scriptstyle \text{Euler-Lagrange } (5)\\
&amp;\scriptstyle \frac{d}{dt} \nabla_{\dot q} \mathcal{L} \scriptstyle ~=~ \nabla_{q} \mathcal{L} &amp; \scriptstyle \text{vectorize } (6)\\
&amp;\scriptstyle \nabla_q \mathcal{L} \scriptstyle ~=~ (\nabla_{\dot q}\nabla_{\dot q}^{\top}\mathcal{L})\ddot q + (\nabla_{q}\nabla_{\dot q}^{\top}\mathcal{L}) \dot q &amp; \scriptstyle \text{expand }\frac{d}{dt} \text{ }(7)\\
&amp;\scriptstyle \ddot q \scriptstyle ~=~ (\nabla_{\dot q}\nabla_{\dot q}^{\top}\mathcal{L})^{-1}[\nabla_q \mathcal{L} - (\nabla_{q}\nabla_{\dot q}^{\top}\mathcal{L})\dot q] &amp; \scriptstyle \text{solve for } \ddot q \text{ }(8)\\
\end{align}\)
</span>
<span id="longEqnWithLargeScript_B" style="display:block; margin-left:auto;margin-right:auto;text-align:center;">
\(\begin{align}
\frac{d}{dt} \frac{\partial \mathcal{L}}{\partial \dot q_j} &amp;= \frac{\partial \mathcal{L}}{\partial q_j} &amp; \text{the Euler-Lagrange equation} \quad (5)\\
\frac{d}{dt} \nabla_{\dot q} \mathcal{L} &amp;= \nabla_{q} \mathcal{L} &amp; \text{switch to vector notation} \quad (6)\\
(\nabla_{\dot q}\nabla_{\dot q}^{\top}\mathcal{L})\ddot q + (\nabla_{q}\nabla_{\dot q}^{\top}\mathcal{L}) \dot q &amp;= \nabla_q \mathcal{L} &amp; \text{expand time derivative }\frac{d}{dt} \quad (7)\\
\ddot q &amp;= (\nabla_{\dot q}\nabla_{\dot q}^{\top}\mathcal{L})^{-1}[\nabla_q \mathcal{L} - (\nabla_{q}\nabla_{\dot q}^{\top}\mathcal{L})\dot q] &amp; \text{matrix inverse to solve for } \ddot q \quad (8)\\
\end{align}\)
</span></p>

<p>For a given set of coordinates \(x_t=(q_t,\dot q_t)\), we now have a method for calculating \(\dot x_t=(\dot q_t,\ddot q_t)\) from a blackbox Lagrangian. We can integrate this quantity to obtain the dynamics of the system. And in the same manner as Hamiltonian Neural Networks, we can learn \(\mathcal{L_{\theta}}\) by differentiating the MSE loss between \(\dot x_t^{\mathcal{L_{\theta}}}\) and \(\dot x_t^{\textrm{true}}\).</p>

<p><strong>Implementation.</strong> If you look closely at Equation 8, you may notice that it involves both the Hessian and the gradient of a neural network during the forward pass of the LNN. This is not a trivial operation, but modern automatic differentiation makes things surprisingly smooth. Written in <a href="https://github.com/google/jax">JAX</a>, Equation 8 is just a few lines of code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q_tt</span> <span class="o">=</span> <span class="p">(</span>
	<span class="n">jax</span><span class="p">.</span><span class="n">numpy</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">lagrangian</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">q</span><span class="p">,</span> <span class="n">q_t</span><span class="p">))</span> <span class="o">@</span> <span class="p">(</span>
		<span class="n">jax</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">lagrangian</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">q</span><span class="p">,</span> <span class="n">q_t</span><span class="p">)</span>
		<span class="o">-</span> <span class="n">jax</span><span class="p">.</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">lagrangian</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)(</span><span class="n">q</span><span class="p">,</span> <span class="n">q_t</span><span class="p">)</span> <span class="o">@</span> <span class="n">q_t</span>
	<span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="learning-real-lagrangians">Learning real Lagrangians</h2>

<p>In our paper, we conduct several experiments to validate this approach. In the first, we show that Lagrangian Neural Networks can learn the dynamics of a double pendulum.</p>

<p><strong>Double pendulum.</strong> The double pendulum is a dynamics problem that regular neural networks struggle to fit because they have no prior for conserving the total energy of the system. It is also a problem where HNNs struggle, since the canonical coordinates of the system are not trivial to compute (see equations 1 and 2 of <a href="https://diego.assencio.com/?index=e5ac36fcb129ce95a61f8e8ce0572dbf">this derivation</a> for example). But in contrast to these baseline methods, Figure 4 shows that LNNs are able to learn the Lagrangian of a double pendulum.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
	<img src="/assets/lagrangian-nns/dblpend_error.png" />
	<div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto;"><b>Figure 4:</b> Learning the dynamics of a double pendulum. Unlike the baseline neural network, our model learns to approximately conserve the total energy of the system. This is a consequence of the strong physical inductive bias of the Euler-Lagrange constraint.</div>
</div>

<p>It’s also interesting to compare qualitative results. In the video below, we use a baseline neural network and an LNN to predict the dynamics of a double pendulum, starting from the same initial state. You’ll notice that both trajectories seem reasonable until the end of the video, when the baseline model shifts to states that have much lower total energies.</p>

<div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:80%">
	<div style="overflow:hidden; padding-top: 40%; position: relative;">
		<iframe style="border: 0;height: 100%;left: 0;position: absolute;top: 0;width: 100%;" src="https://www.youtube.com/embed/ulQKNtTEuJI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
	</div>
	<div class="thecap" style="text-align:center; display: block; margin-left: auto; margin-right: auto; width:60%"><b>Figure 5:</b> Dynamics predictions of a baseline (left) versus an LNN (right)</div>
</div>

<p><strong>Relativistic particle.</strong> Another system we considered was a particle of mass \(m=1\) moving at a relativistic velocity through a potential \(g\) with \(c=1\). The Lagrangian of the system is \(\mathcal{L} = ((1 - \dot{q}^2)^{-1/2} - 1) + g q\) and it is interesting because existing Hamiltonian and Lagrangian learning approaches fail. HNNs fail because the canonical momenta of the system are hard to compute. Deep Lagrangian Networks<sup id="fnref:fn3" role="doc-noteref"><a href="#fn:fn3" class="footnote">2</a></sup> fail because they make restrictive assumptions about the form of the Lagrangian.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:80%">
	<img src="/assets/lagrangian-nns/relativistic.png" />
	<div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto;"><b>Figure 6:</b> Learning the dynamics of a relativistic particle. In the first plot (a), an HNN model fails to model the system because the default coordinates are non-canonical. In the second plot (b), we provide the HNN with proper canonical coordinates and it succeeds. In the third plot (c), we show that an LNN can fit the data even in the absence of canonical coordinates.</div>
</div>

<h2 id="related-work">Related Work</h2>

<p><strong>Learning invariant quantities.</strong> This approach is similar in spirit to <a href="https://greydanus.github.io/2019/05/15/hamiltonian-nns/">Hamiltonian Neural Networks</a> (HNNs) and Hamiltonian Generative Networks<sup id="fnref:fn2" role="doc-noteref"><a href="#fn:fn2" class="footnote">3</a></sup> (HGNs). In fact, this blog post was written as a compliment to the original HNN post and it has the same fundamental motivations. Unlike these previous works, our aim here is to learn a Lagrangian rather than a Hamiltonian so as not to restrict the inputs to being canonical coordinates. It’s worth noting that once we learn a Lagrangian, we can always use it to obtain the value of a Hamiltonian using the <a href="https://en.wikipedia.org/wiki/Legendre_transformation">Legendre transformation</a>.</p>

<p><strong>Deep Lagrangian Networks (DeLaN, ICLR’19).</strong> Another closely related work is Deep Lagrangian Networks<sup id="fnref:fn3:1" role="doc-noteref"><a href="#fn:fn3" class="footnote">2</a></sup> in which the authors show how to learn specific types of Lagrangian systems. They assume that the kinetic energy is an inner product of the velocity, which works well for rigid body dynamics such as those in robotics. However, there are many physical systems that do not have this specific form. Some simple examples include a charged particle in a magnetic field or a fast-moving object with relativistic corrections. We see LNNs as a complement to DeLaNs in that they cover the cases where DeLaNs struggle but are less amenable to robotics applications.</p>

<h2 id="closing-thoughts">Closing Thoughts</h2>

<!-- Looking forward, we continue to be excited about the connection between machine learning and the principle of stationary action. One thing we'd like to try is to write a loss function that _is_ the action \\(S\\) and then minimize it with gradient descent to obtain dynamics. On a similar note, we'd like to think more about the connection between existing neural network training dynamics and the principle of least action. One of Yann LeCun's most beautiful papers, for example, is a derivation of backpropagation via the Euler-Lagrange constraint. This may be the proper way to speak about optimization dynamics such as catastrophic forgetting and deep double descent. -->

<p>The principle of stationary action is a unifying force in physics. It represents a consistent “law of the universe” which holds true in every system humans have ever studied: from the very small<sup id="fnref:fn1:1" role="doc-noteref"><a href="#fn:fn1" class="footnote">1</a></sup> to the very large, from the very slow to the very fast. Lagrangian Neural Networks represent a different sort of unification. They aim to strengthen the connection between real-world data and the underlying physical constraints that it obeys. This gives LNNs their own sort of beauty, a beauty that Lagrange himself may have admired.</p>

<h2 id="footnotes">Footnotes</h2>

<script>
    function getBrowserSize(){
       var w, h;

         if(typeof window.innerWidth != 'undefined')
         {
          w = window.innerWidth; //other browsers
          h = window.innerHeight;
         } 
         else if(typeof document.documentElement != 'undefined' && typeof      document.documentElement.clientWidth != 'undefined' && document.documentElement.clientWidth != 0) 
         {
          w =  document.documentElement.clientWidth; //IE
          h = document.documentElement.clientHeight;
         }
         else{
          w = document.body.clientWidth; //IE
          h = document.body.clientHeight;
         }
       return {'width':w, 'height': h};
}

if(parseInt(getBrowserSize().width) < 800){
 document.getElementById("longEqnWithLargeScript_A").style.display = "none";
}
if(parseInt(getBrowserSize().width) > 800){
 document.getElementById("longEqnWithSmallScript_A").style.display = "none";
}

if(parseInt(getBrowserSize().width) < 800){
 document.getElementById("longEqnWithLargeScript_B").style.display = "none";
}
if(parseInt(getBrowserSize().width) > 800){
 document.getElementById("longEqnWithSmallScript_B").style.display = "none";
}
</script>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:fn1" role="doc-endnote">
      <p>Here \(e^{-S/h}\) is actually the probability of a particular path occurring. Because \(h\) is small, we usually only observe the minimum value of \(S\) on large scales. See <a href="https://www.feynmanlectures.caltech.edu/II_19.html">Feynman lecture 19</a> for more on this. <a href="#fnref:fn1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:fn1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:fn3" role="doc-endnote">
      <p>Lutter, M., Ritter, C., and Peters, J. <a href="https://arxiv.org/abs/1907.04490">Deep lagrangian networks: Using physics as model prior for deep learning</a>, <em>International Conference on Learning Representations</em>, 2019. <a href="#fnref:fn3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:fn3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:fn2" role="doc-endnote">
      <p>Toth, P., Rezende, D. J., Jaegle, A., Racanière, S., Botev, A., &amp; Higgins, I. <a href="https://arxiv.org/abs/1909.13789">Hamiltonian Generative Networks</a>, <em>International Conference on Learning Representations</em>, 2020. <a href="#fnref:fn2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <!-- disqus comments -->
 
  <div id="disqus_thread"></div>
  <script>
      /**
       *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
       *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
       */
      /*
      var disqus_config = function () {
          this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
      };
      */
      (function() {  // DON'T EDIT BELOW THIS LINE
          var d = document, s = d.createElement('script');
          
          s.src = '//greydanus-blog.disqus.com/embed.js';
          
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  


  
</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Natural Intelligence</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>Natural Intelligence</li>
        <!-- <li><a href="mailto:greydanus.17(at)gmail.com">greydanus.17(at)gmail.com</a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/greydanus">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">greydanus</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/samgreydanus">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">samgreydanus</span>
          </a>
        </li>
        <li>
          <a href="../../../../feed.xml">
            <span class="icon rss">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="#C2C2C2" d="M6.503 20.752c0 1.794-1.456 3.248-3.251 3.248-1.796 0-3.252-1.454-3.252-3.248 0-1.794 1.456-3.248 3.252-3.248 1.795.001 3.251 1.454 3.251 3.248zm-6.503-12.572v4.811c6.05.062 10.96 4.966 11.022 11.009h4.817c-.062-8.71-7.118-15.758-15.839-15.82zm0-3.368c10.58.046 19.152 8.594 19.183 19.188h4.817c-.03-13.231-10.755-23.954-24-24v4.812z"/></svg>
            </span>
            <span class="username">RSS</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">A science blog by Sam Greydanus</p>
    </div>

  </div>

</footer>


    </body>
</html>