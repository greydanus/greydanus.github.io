<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Piecewise-constant Neural ODEs</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="A science blog by Sam Greydanus">
    <link rel="canonical" href="http://greydanus.github.io/2021/06/11/piecewise-nodes/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Natural Intelligence posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/assets/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-72311215-6', 'auto');
      ga('send', 'pageview');

    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    
    <!-- Allow RSS subscribers, as described in https://dzhavat.github.io/2020/01/19/adding-an-rss-feed-to-github-pages.html -->
    <link rel="alternate" type="application/atom+xml" title="Sam Greydanus’ blog" href="/feed.xml">

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:11px; margin-right:10px;">
    <img src="/assets/oak.png" width="50px">
    </div>

    <div style="float:left; margin-top:4px; margin-right:10px;">
      <a class="site-title" style="padding-top:0px; padding-bottom:0px;" href="/ ">Natural Intelligence</a>
      <!--<p style="font-size:14px;">‎‎ㅤㅤㅤ&zwnj;</p> This is very hacky but I couldn't find a better way -->
      <br>
      <p style="padding-bottom:6px; font-size:15px;font-weight: 300;">A science blog by Sam Greydanus</p>
    </div>

<!--     <a class="site-title" style="padding-top:8px; font-size:28px" href="/ ">Natural Intelligence</a> -->
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger" style="margin-bottom:0px; padding-top:10px; font-size:13px;font-weight: 300;">
<!--         <a class="page-link" href="/ ">Home</a> -->
<!--         
          <a class="page-link" href="/assets/2017-12-23-neurips17/">A Review of NeurIPS</a>
        
          <a class="page-link" href="/2022-04-08-sparsity/">The Story of Sparsity in Neural Networks</a>
        
          <a class="page-link" href="/2022-05-18-studying-growth/">Studying Growth with Cellular Automata</a>
        
          
        
          <a class="page-link" href="/idea-of-god/">The Idea of God</a>
        
          
        
          <a class="page-link" href="/justification/">Trimmer on Justification</a>
        
          <a class="page-link" href="/papers/">Interesting papers</a>
        
          
         -->
        <a class="page-link" style="margin-bottom: 0px; padding-bottom:0px;" href="../../../../index.html">Home</a>
<!--         <a class="page-link" style="margin-bottom: 0px; padding-bottom:0px;" href="../../../../about.html">About me (old)</a> -->
        <a class="page-link" style="margin-bottom: 0px; padding-bottom:0px;" href="/about_me">About me</a>
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Piecewise-constant Neural ODEs</h1>
    <p class="meta">Jun 11, 2021 • Sam Greydanus, Stefan Lee, and Alan Fern</p>
  </header>

  <article class="post-content">
  <div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:99.9%">
  <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;padding-right:10px;">
    <video id="video_sim" style="width:100%;min-width:250px;">
    	<source src="/assets/piecewise-nodes/video_simulator_2.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_sim_button" onclick="playPauseSim()">Play</button> 
    <div style="text-align: left;margin-left:10px;margin-right:10px;">Using model-based planning to play billiards. The goal is to impart the tan cue ball with an initial velocity so as to move the blue ball to the black target.</div>
  </div>
  <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;padding-right:10px;">
    <video id="video_base" style="width:100%;min-width:250px;">
    	<source src="/assets/piecewise-nodes/video_base_2.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_base_button" onclick="playPauseBase()">Play</button> 
    <div style="text-align:left;margin-left:10px;margin-right:10px;">A baseline ODE-RNN trained on billiards dynamics can also be used for model-based planning. It's inefficient because it has to "tick" at a constant rate.</div>
  </div>
   <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;">
    <video id="video_ours" style="width:100%;min-width:250px;">
    	<source src="/assets/piecewise-nodes/video_ours_2.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_ours_button" onclick="playPauseOurs()">Play</button> 
    <div style="text-align:left;margin-left:10px;margin-right:10px;">By contrast, our model, trained on the same task, can perform planning in many fewer steps by jumping over spans of time where motion is predictable.</div>
  </div>
</div>

<script> 
function playPauseSim() { 
  var video = document.getElementById("video_sim"); 
  var button = document.getElementById("video_sim_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 

function playPauseBase() { 
  var video = document.getElementById("video_base"); 
  var button = document.getElementById("video_base_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 

function playPauseOurs() { 
  var video = document.getElementById("video_ours"); 
  var button = document.getElementById("video_ours_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 
</script>

<div style="display: block; margin-left: auto; margin-right: auto; width:100%; text-align:center;">
	<a href="https://arxiv.org/abs/2106.06621" id="linkbutton" target="_blank">Read the paper</a>
	<a href="https://github.com/greydanus/piecewise_node#run-in-your-browser" id="linkbutton" target="_blank"><span class="colab-span">Run</span> in browser</a>
	<a href="https://github.com/greydanus/piecewise_node" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="change-it-is-said-happens-slowly-and-then-all-at-once">Change, it is said, happens slowly and then all at once…</h2>

<p>Billiards balls move across a table before colliding and changing trajectories; water molecules cool slowly and then undergo a rapid phase transition into ice; and economic systems enjoy periods of stability interspersed with abrupt market downturns. That is to say, many time series exhibit periods of relatively homogeneous change divided by important events. Despite this, recurrent neural networks (RNNs), popular for time series modeling, treat time in uniform intervals – potentially wasting prediction resources on long intervals of relatively constant change.</p>

<p>A recent family of models called <a href="https://arxiv.org/abs/1806.07366">Neural ODEs</a> has attracted interest as a means of mitigating these problems. They parameterize the <em>time derivative</em> of a hidden state with a neural network and then integrate it over arbitrary amounts of time. This allows them to treat time as a continuous variable. Integration can even be performed using adaptive integrators like <a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods">Runge-Kutta</a>, thus allocating more compute to difficult state transitions.</p>

<p>Adaptive integration is especially attractive in scenarios where “key events” are separated by variable amounts of time. In the game of billiards, these key events may consist of collisions between balls, walls, and pockets. Between these events, the balls simply undergo linear motion. That motion is not difficult to predict, but it is non-trivial for a model to learn to skip over it so as to focus on the more chaotic dynamics of collisions; this requires a model to employ some notion of <a href="https://www.sciencedirect.com/science/article/pii/S0004370299000521"><em>temporal abstraction</em></a>. This problem is not unique to billiards. The same challenge occurs in robotics, where a robot arm occasionally interacts with external objects at varying intervals. It may also occur in financial markets, scientific timeseries, and other environments where change happens at a variable rate.</p>

<h2 id="towards-temporally-abstract-hidden-state-dynamics">Towards temporally-abstract hidden state dynamics</h2>

<p>In this post, I am going to introduce a special case of Neural ODEs that my research group has been experimenting with recently. The core idea is to restrict the hidden state of a Neural ODE so that it has locally-linear dynamics. The benefit of such a model is that it can be integrated <em>exactly</em> using Euler integration, and it can also be integrated <em>adaptively</em> because we allow these locally-linear dynamics to extend over variable-sized durations of time. Like RNNS and Neural ODEs, our model uses a hidden state \(h\) to summarize knowledge about the world at a given point in time. Also, it performs updates on this hidden state using cell updates (eg. with vanilla, LSTM, or GRU cells). But our model differs from existing models in that the amount of simulation time that occurs between cell updates is not fixed. Rather, it changes according to the variable \(\Delta t\), which is itself predicted.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/piecewise-nodes/hero.png" style="width:100%" />
</div>

<p>Our model also predicts a <em>hidden state velocity</em>, \(\dot h\), at each cell update; this enables us to evolve the hidden state dynamics continuously over time according to \(h(t+\Delta t) = h + \dot h \Delta t\). In other words, the hidden state velocity allows us to parameterize <em>the locally-linear dynamics of the hidden state</em>. Thus when our model needs to simulate long spans of homogeneous change (eg, a billiard ball undergoing linear motion), it can do so with a single cell update.</p>

<p>In order to compare our model to existing timeseries models (RNNs and Neural ODEs), we used both of them to model a series of simple physics problems including the collisions of two billiards balls. We found that our jumpy model was able to learn these dynamics at least as well as the baseline while using a fraction of the forward simulation steps. This makes it a great candidate for model-based planning because it can predict the outcome of taking an action much more quickly than a baseline model. And since the hidden-state dynamics are piecewise-linear over time, we can solve for the hidden state at arbitrary points along a trajectory. This allows us to simulate the dynamics <em>at a higher temporal resolution than the original training data</em>:</p>

<div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:99.9%">
    <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;padding-right:10px;">
    <video id="video_sim2" style="width:100%;min-width:250px;">
      <source src="/assets/piecewise-nodes/video_sim_2.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_sim2_button" onclick="playPauseSim2()">Play</button> 
    <div style="text-align: left;margin-left:10px;margin-right:10px;">This video will give you a sense of the underlying temporal resolution of the billiards dataset on which we trained the model.</div>
  </div>
  <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;">
    <video id="video_interp" style="width:100%;min-width:250px;">
    	<source src="/assets/piecewise-nodes/video_interp_2.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_interp_button" onclick="playPauseInterp()">Play</button> 
    <div style="text-align:left;margin-left:10px;">This video shows how we can use our model to generate simulations at a higher temporal resolution than that of the original simulator. 
<!--     We can do this because the latent dynamics of the model are continuous and piecewise-linear in time. -->
  </div>
  </div>
</div>

<script> 
function playPauseSim2() { 
  var video = document.getElementById("video_sim2"); 
  var button = document.getElementById("video_sim2_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 

function playPauseJumpy2() { 
  var video = document.getElementById("video_jumpy2"); 
  var button = document.getElementById("video_jumpy2_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 

function playPauseInterp() { 
  var video = document.getElementById("video_interp"); 
  var button = document.getElementById("video_interp_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 
</script>

<p>I am going to give more specific examples of how our model improves over regular timeseries models later. But first we need to talk about what these timeseries models are good at and why they are worth improving in the first place.</p>

<h2 id="the-value-of-timeseries-models">The value of timeseries models</h2>

<p>Neural network-based timeseries models like RNNs and Neural ODEs are interesting because they can learn complex, long-range structure in time series data simply by predicting one point at a time. For example, if you train them on observations of a robot arm, you can use them to generate realistic paths that the arm might take.</p>

<p>One of the things that makes these models so flexible is that they use a hidden vector \(h\) to store memories of past observations. And they can <em>learn</em> to read, write, and erase information from \(h\) in order to make accurate predictions about the future. RNNs do this in discrete steps whereas Neural ODEs permit hidden state dynamics to be continuous in time. Both models are Turing-complete and, unlike other models that are Turing-complete (eg. HMMs or FSMs), they can learn and operate on noisy, high-dimensional data. Here is an incomplete list of things people have trained these models (mostly RNNs) to do:</p>
<ul>
  <li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-">Translate text from one language to another</a></li>
  <li><a href="https://openai.com/blog/solving-rubiks-cube/">Control a robot hand in order to solve a Rubik’s Cube</a></li>
  <li><a href="https://rdcu.be/bVI7G">Defeat professional human gamers in StarCraft</a></li>
  <li><a href="https://openaccess.thecvf.com/content_cvpr_2016/html/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.html">Caption images</a></li>
  <li><a href="https://arxiv.org/abs/1308.0850">Generate realistic handwriting</a></li>
  <li><a href="https://www.isca-speech.org/archive/interspeech_2014/i14_1964.html">Convert text to speech</a></li>
  <li><a href="http://www.jmlr.org/proceedings/papers/v48/amodei16.html">Convert speech to text</a></li>
  <li><a href="https://arxiv.org/abs/1704.03477">Sketch simple images</a></li>
  <li><a href="https://greydanus.github.io/2017/01/07/enigma-rnn/">Learn the Enigma cipher</a> [one of my first projects :D]</li>
  <li><a href="https://arxiv.org/abs/1907.03907">Predict patient ICU data such as aiastolic arterial blood pressure</a> [Neural ODEs]</li>
</ul>

<h2 id="limitations-of-these-sequence-models">Limitations of these sequence models</h2>

<p>Let’s begin with the limitations of RNNs, use them to motivate Neural ODEs, and then discuss the contexts in which even Neural ODEs have shortcomings. The first and most serious limitation of RNNs is that they can only predict the future by way of discrete, uniform “ticks”.</p>

<p><strong>Uniform ticks.</strong> At each tick they make one observation of the world, perform one read-erase-write operation on their memory, and output one state vector. This seems too rigid. We wouldn’t divide our perception of the world into uniform segments of, say, ten minutes. This would be silly because the important events of our daily routines are not spaced equally apart.</p>

<p>Consider the game of billiards. When you prepare to strike the cue ball, you imagine how it will collide with other balls and eventually send one of them into a pocket. And when you do this, you do not think about the constant motion of the cue ball as it rolls across the table. Instead, you think about the near-instantaneous collisions between the cue ball, walls, and pockets. Since these collisions are separated by variable amounts of time, making this plan requires that you jump from one collision event to another without much regard for the intervening duration. This is something that RNNs cannot do.</p>

<div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:99.9%">
    <div style="width:100%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;padding-right:10px;">
    <video id="video_pool" style="width:100%;min-width:250px;">
      <source src="/assets/piecewise-nodes/pool_shot.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_pool_button" onclick="playPausePool()">Play</button> 
    <div style="text-align: left;">A professional pool player making a remarkable shot. We'll never know exactly what was going through his head when he did this, but we can say at the very least he was planning over a sequence of collisions. An RNN, by contrast, would focus most of its compute on simulating the linear motion of the ball in between collisions.</div>
  </div>
</div>

<script> 
function playPausePool() { 
  var video = document.getElementById("video_pool"); 
  var button = document.getElementById("video_pool_button");
  if (video.paused) {
    video.play();
  button.textContent = "Pause";}
  else {
    video.pause(); 
  button.textContent = "Play";}
} 
</script>

<p><strong>Discrete time steps.</strong> Another issue with RNNs is that they perceive time as a series of discrete “time steps” that connect neighboring states. Since time is actually a continuous variable – it has a definite value even in between RNN ticks – we really should use models that treat it as such. In other words, when we ask our model what the world looked like at time \( t=1.42\) seconds, it should not have to locate the two ticks that are nearest in time and then interpolate between them, as is the case with RNNs. Rather, it should be able to give a well-defined answer.</p>

<p><strong>Avoiding discrete, uniform timesteps with Neural ODEs.</strong> These problems represent some of the core motivations for Neural ODEs. Neural ODEs parameterize the time derivative of the hidden state and, when combined with an ODE integrator, can be used to model dynamical systems where time is a continuous variable. These models represent a young and rapidly expanding area of machine learning research. One unresolved challenge with these models is getting them to run efficiently with adaptive ODE integrators…</p>

<p>The problem is that adaptive ODE integrators must perform several function evaluations in order to estimate local curvature when performing an integration step. The curvature information determines how far the integrator can step forward in time, subject to a constant error budget. This is a particularly serious issue in the context of neural networks, which may have very irregular local curvatures at initialization. A single Neural ODE training step can take up to five times longer to evaluate than a comparable RNN architecture, making it challenging to scale these models.<sup id="fnref:fn7" role="doc-noteref"><a href="#fn:fn7" class="footnote">1</a></sup> The curvature problem has, in fact, already motivated some work on regularizing the curvature of Neural ODEs so as to train them more efficiently.<sup id="fnref:fn6" role="doc-noteref"><a href="#fn:fn6" class="footnote">2</a></sup> But even with regularization, these models are more difficult to train than RNNs. Furthermore, there are many tasks where regularizing curvature is counterproductive, for example, modeling elastic collisions between two bodies.<sup id="fnref:fn18" role="doc-noteref"><a href="#fn:fn18" class="footnote">3</a></sup></p>

<h2 id="our-results">Our Results</h2>

<p>Our work on piecewise-constant Neural ODEs was an attempt to fix these issues. Our model can jump over different durations of time and can tick more often when a lot is happening and less often otherwise. As I explained earlier, these models are different from regular RNNs in that they predict a hidden state velocity in addition to a hidden state. Taken together, these two quantities represent a linear dynamics function in the RNN’s latent space. A second modification is to have the model predict the duration of time \(\Delta t\) over which its dynamics functions are valid. In some cases, when change is happening at a constant rate, this value can be quite large.</p>

<p><strong>Learning linear motion.</strong> To show this more clearly, we conducted a simple toy experiment. We created a toy dataset of perfectly linear motion and checked to see whether our model would learn to summarize the whole thing in one step. As the figure below shows, it learned to do exactly that. Meanwhile, the regular RNN had to summarize the same motion in a series of tiny steps.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/piecewise-nodes/lines.png" style="width:100%" />
</div>

<p><strong>Learning a change of basis.</strong> Physicists will tell you that the way a system changes over time is only linear with respect to a particular coordinate system. For example, an object undergoing constant circular motion has nonlinear dynamics when we use Cartesian coordinates, but linear dynamics when we use polar coordinates. That’s why physicists use different coordinates to describe different physical systems: <u><i>all else being equal, the best coordinates are those that are maximally linear with respect to the dynamics.</i></u></p>

<p>Since our model forces dynamics to be linear in latent space, the encoder and decoder layers naturally learn to transform input data into a basis where the dynamics are linear. For example, when we train our model on a dataset of circular trajectories represented in Cartesian coordinates, it learns to summarize such trajectories in a single step. This implies that our model has learned a Cartesian-to-Polar change of basis.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/piecewise-nodes/circles.png" style="width:100%" />
</div>

<p><strong>Learning from pixel videos.</strong> Our model can learn more complicated change-of-basis functions as well. Later in the paper, we trained our model on pixel observations of two billiards balls. The pixel “coordinate system” is extremely nonlinear with respect to the linear motion of the two balls. And yet our model was able to predict the dynamics of the system far more effectively than the baseline model, while using three times fewer “ticks”. The fact that our model could make jumpy predictions on this dataset implies that it found a basis where the billiards dynamics were linear for significant durations of time – something that is strictly impossible in a pixel basis.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/piecewise-nodes/pixel_billiards.png" style="width:100%" />
</div>

<p>In fact, we suspect that forcing dynamics to be linear in latent space actually biased our model to find linear dynamics. We hypothesize that the baseline model performed worse on this task because it had no such inductive bias. This is generally a good inductive bias to build into a model because most real-world dynamics can be approximated with piecewise-linear functions</p>

<h2 id="planning">Planning</h2>

<p>One of the reasons we originally set out to build this model was that we wanted to use it for planning. We were struck by the fact that many events one would want to plan over – collisions, in the case of billiards – are separated by variable durations of time. We suspected that a model that could jump through uneventful time intervals would be particularly effective at planning because it could plan over the events that really mattered (eg, collisions).</p>

<p>In order to test this hypothesis, we compared our model to RNN and ODE-RNN baselines on a simple planning task in the billiards environment. The goal was to impart one ball, the “cue ball” (visualized in tan) with an initial velocity such that it would collide with the second ball and the second ball would ultimately enter a target region (visualized in black). You can see videos of such plans at the beginning of this post.</p>

<p>We found that our model used at least half the wall time of the baselines and produced plans with a higher probability of success. These results are preliminary – and part of ongoing work – but they do support our initial hypothesis.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Simulator</th>
      <th style="text-align: center">Baseline RNN</th>
      <th style="text-align: center">Baseline ODE-RNN</th>
      <th style="text-align: center">Our model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">85.2%</td>
      <td style="text-align: center">55.6%</td>
      <td style="text-align: center">17.0%</td>
      <td style="text-align: center">61.6%</td>
    </tr>
  </tbody>
</table>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/piecewise-nodes/planning2d.png" style="width:45%" />
</div>

<!-- Quite a few researchers have wrestled with the fact that RNNs tick through time at a uniform rate. So there are a number of recent projects that aim to make RNNs more temporally abstract. Our work is related, and hopefully complementary, to these approaches. -->

<h2 id="related-work-aside-from-rnns-and-neural-odes">Related work aside from RNNs and Neural ODEs</h2>

<!-- Quite a few researchers have wrestled with the same limitations of RNNs that we have. So there are a number of related works aimed at solving the same issues. Among the most relevant of these works is a family of models called Neural ODEs.

**Neural ODEs.** The past few years have seen a surge of interest in these models. The basic idea of a Neural ODE is to parameterize the derivative of some variable with a neural network and then integrate it. For example, if you wanted to obtain the continuous-time dynamics of a hidden state \\(h_t\\), you would start by setting \\(\frac{\partial h_t}{\partial t}=f_{NN}(h_t)\\) where \\(f_{NN}\\) is a neural network. Then you could integrate that function over time to get dynamics:

$$ h_{t_1} ~=~ h_{t_0} + \int_{t_0}^{t_1} f_{NN}(h_t) ~~ dt $$

One of the remarkable things about this approach is that you can literally integrate your model with an ODE integrator, eg. ``scipy.integrate.solve_ivp``. Likewise, you can backpropagate error signals to your model with a second call to the integrator.

**Connection to our work.** Neural ODEs can be integrated _adaptively_; in other words, the size of the integration step can be made proportional to the local curvature. So in theory, if one were to regularize a Neural ODE to have very low curvature, one might be able to see the same jumpy behavior that we document in Jumpy RNNs. In practice, figuring out how to properly regularize the curvature of these models remains an open question.[^fn6] And current versions of Neural ODEs tend to be _more_ computationally demanding to evaluate than regular RNN models. In a recent paper about modeling RNN hidden state dynamics with ODEs[^fn7], for example, the authors mention that the ODE forward passes took 60% -- 120% longer than standard RNNs since they had to be continuously solved even when no observations were occurring.

Jumpy RNNs resemble Neural ODEs in that they parameterize the derivative of a hidden state. But unlike Neural ODEs, Jumpy RNNs assume that the function being integrated is piecewise-linear and they do not require an ODE solver. The local linearity assumption makes our model extremely efficient to integrate over long spans of time -- much more efficient, for example, than a baseline RNN, and by extension, a Neural ODE.[^fn0]

**Other related works.**  -->

<p>Quite a few researchers have wrestled with the same limitations of RNNs and Neural ODEs that we have in this post. For example, there are a number of other RNN-based models designed with temporal abstraction in mind: Koutnik et al. (2014)<sup id="fnref:fn1" role="doc-noteref"><a href="#fn:fn1" class="footnote">4</a></sup> proposed dividing an RNN internal state into groups and only performing cell updates on the \(i^{th}\) group after \(2^{i-1}\) time steps. More recent works have aimed to make this hierarchical structure more adaptive, either by data-specific rules<sup id="fnref:fn2" role="doc-noteref"><a href="#fn:fn2" class="footnote">5</a></sup> or by a learning  mechanism<sup id="fnref:fn3" role="doc-noteref"><a href="#fn:fn3" class="footnote">6</a></sup>. But although these hierarchical recurrent models can model data at different timescales, they still must perform cell updates at every time step in a sequence and cannot jump over regions of homogeneous change.</p>

<p>For a discussion of these methods (and many others), check out <a href="https://arxiv.org/abs/2106.06621">the full paper</a>, which we link to at the top of this post.
<!-- Another relevant work from reinforcement learning is "Embed to Control"[^fn5]. This work is similar to ours in that it assumes that dynamics are linear in latent space. But unlike our work, the E2C model performs inference over discrete, uniform time steps and does not learn a jumpy behavior. --></p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>Neural networks are already a widely used tool, but they still have fundamental limitations. In this post, we reckoned with the fact that they struggle at adaptive timestepping and the computational expense of integration. In order to make RNNs and Neural ODEs more useful in more contexts, it is essential to find solutions to such restrictions. With this in mind, we proposed a PC-ODE model which can skip over long durations of comparatively homogeneous change and focus on pivotal events as the need arises. We hope that this line of work will lead to models that can represent time more efficiently and flexibly.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:fn7" role="doc-endnote">
      <p>Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. <a href="https://papers.nips.cc/paper/8773-latent-ordinary-differential-equations-for-irregularly-sampled-time-series">Latent odes for irregularly-sampled time series</a>. <em>Advances in Neural Information Processing Systems</em>, 2019. <a href="#fnref:fn7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn6" role="doc-endnote">
      <p>Chris Finlay, Jörn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman. <a href="https://arxiv.org/abs/2002.02798">How to train your neural ode: the world of jacobian and kinetic regularization</a>. <em>International Conference on Machine Learning</em>, 2020. <a href="#fnref:fn6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn18" role="doc-endnote">
      <p>Jia, Junteng, and Austin R. Benson. <a href="https://papers.nips.cc/paper/2019/hash/59b1deff341edb0b76ace57820cef237-Abstract.html">Neural jump stochastic differential equations</a>. Neural Information Processing Systems, 2019 <a href="#fnref:fn18" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn1" role="doc-endnote">
      <p>Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. <a href="https://arxiv.org/abs/1402.3511">A Clockwork RNN</a>. <em>International Conference on Machine Learning</em>, pp. 1863–1871, 2014. <a href="#fnref:fn1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn2" role="doc-endnote">
      <p>Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W Black. <a href="https://arxiv.org/abs/1511.04586">Character-based neural machine translation</a>. <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</em>, 2015. <a href="#fnref:fn2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn3" role="doc-endnote">
      <p>Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. <a href="https://arxiv.org/abs/1609.01704">Hierarchical multiscale recurrent neural networks</a>. <em>5th International Conference on Learning Representations</em>, ICLR 2017. <a href="#fnref:fn3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <!-- disqus comments -->
 
  <div id="disqus_thread"></div>
  <script>
      /**
       *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
       *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
       */
      /*
      var disqus_config = function () {
          this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
      };
      */
      (function() {  // DON'T EDIT BELOW THIS LINE
          var d = document, s = d.createElement('script');
          
          s.src = '//greydanus-blog.disqus.com/embed.js';
          
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  


  
</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Natural Intelligence</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>Natural Intelligence</li>
        <!-- <li><a href="mailto:greydanus.17(at)gmail.com">greydanus.17(at)gmail.com</a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/greydanus">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">greydanus</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/samgreydanus">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">samgreydanus</span>
          </a>
        </li>
        <li>
          <a href="../../../../feed.xml">
            <span class="icon rss">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="#C2C2C2" d="M6.503 20.752c0 1.794-1.456 3.248-3.251 3.248-1.796 0-3.252-1.454-3.252-3.248 0-1.794 1.456-3.248 3.252-3.248 1.795.001 3.251 1.454 3.251 3.248zm-6.503-12.572v4.811c6.05.062 10.96 4.966 11.022 11.009h4.817c-.062-8.71-7.118-15.758-15.839-15.82zm0-3.368c10.58.046 19.152 8.594 19.183 19.188h4.817c-.03-13.231-10.755-23.954-24-24v4.812z"/></svg>
            </span>
            <span class="username">RSS</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">A science blog by Sam Greydanus</p>
    </div>

  </div>

</footer>


    </body>
</html>