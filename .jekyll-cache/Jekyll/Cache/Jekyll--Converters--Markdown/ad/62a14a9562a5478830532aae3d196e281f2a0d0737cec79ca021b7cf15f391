I"+<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hero.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  Dissipative HNNs (D-HNNs) output two scalar functions, denoted here by <i><b>H</b></i> and <i><b>D</b></i>. The first of these two, <i><b>H</b></i>, is the Hamiltonian. We use its symplectic gradient to model energy-conserving dynamics. The second, <i><b>D</b></i>, is the Rayleigh dissipation function. It models the dissipative component of the dynamics of a physical system. The addition of the dissipation function is what sets this model apart from Hamiltonian Neural Networks; it allows D-HNNs to model systems where energy is not quite conserved – as, for example, in the case of a damped mass-spring system.
  </div>
</div>

<div style="display: block; margin-left: auto; margin-right:auto; width:100%; text-align:center;">
  <a href="https://arxiv.org/abs/2201.10085" id="linkbutton" target="_blank">Read the paper</a>
  <a href="https://github.com/greydanus/dissipative_hnns" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="a-sea-of-change">A sea of change</h2>

<p>We are immersed in a complex, dynamic world where change is the only constant. And yet there are certain patterns to this change that suggest natural laws. These laws include conservation of mass, energy, and momentum. Taken together, they constitute a powerful simplifying constraint on reality. Indeed, physics tells us that a small set of laws and their associated invariances are at the heart of all natural phenomena. Whether we are studying weather, ocean currents, earthquakes, or molecular interactions, we should take care to respect these laws. And when we apply learning algorithms to these domains, we should ensure that they, too, respect these laws.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/sea_of_change.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  We live in a sea of change. Regardless of how complex a system's dynamics are, they can always be written as the sum of dissipative dynamics and conservative dynamics. By construction D-HNNs, are forced to learn these dynamics separately.
  </div>
</div>

<p>Recent work has shown that neural networks can learn such symmetries directly from data using Hamiltonian Neural Networks (HNNs), Lagrangian Neural Networks, and a growing class of related models. But HNNs and LNNs both struggle when trained on datasets where energy is not conserved. In this post, we’ll look at an extension of HNNs which can be trained on physics data where energy is not strictly conserved. We call this class of models “Dissipative HNNs” and the core idea is use a neural network to parameterize both a Hamiltonian <em>and</em> a Rayleigh dissipation function. During training, the Hamiltonian is used to fit conservative (rotational) dynamics whereas the Rayleigh function is used to fit dissipative dynamics.</p>

<p>Taken together, they represent an implicit Helmholtz decomposition. In other words, they can separate any vector field (representing, in this case, the dynamics of a physical system), into rotational and irrotational components. In the paper, we apply D-HNNs to synthetic mass-spring data, real world pendulum data, and real-world ocean current data. We’ll show some of the highlights of those experiments in this post.</p>

<h2 id="how-it-works">How it works</h2>

<p>The Hamiltonian \(\mathcal{H}(\textbf{q},\textbf{p})\) is scalar function such that
\(\frac{\partial \mathcal{H}}{\partial \textbf{p}} = \frac{\partial \textbf{q}}{dt},  -\frac{\partial \mathcal{H}}{\partial \textbf{q}} = \frac{\partial \textbf{p}}{dt}\)</p>

<p>The Rayleigh dissipation function \(\mathcal{D}(\dot{\textbf{q}})\) is a scalar function that provides a way to account for generalized, velocity-dependent dissipative forces—such as friction—in the context of Hamiltonian mechanics. The equation below shows the Rayleigh function for linear, velocity-dependent dissipation. Here $\rho$ is a constant and $\dot q$ is the velocity coordinate.</p>

\[\mathcal{D} = \frac{1}{2}\rho\dot{q}^2\]

<p>Nonlinear, velocity-dependent dissipation forces \(F_i\) within the system can be accounted for as long as they are defined as 
\(F^{f}_i = -\frac{\partial \mathcal{D}(\dot{\textbf{q}})}{\partial \dot{\textbf{q}}}\)</p>

<h2 id="experiments">Experiments</h2>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/dampedspring.jpg" style="width:80%" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  Training a Dissipative Hamiltonian Neural Network (D-HNN) and several baseline models on a damped spring task. <b>Row 1.</b> We decompose the original dataset by interpolating the training data onto a grid with a nearest-neighbors approach and then decomposing the field into two components. In practice, we accomplish this using a few hundred iterations of the Gauss-Seidel method to solve Poisson's equation. <b>Row 2.</b> We train a D-HNN on the same data. The first half of this model looks exactly like an HNN. The second half also looks like an HNN, except we use the gradient of the scalar field directly rather than rearranging it according to Hamilton's equations. This gives us a trainable \textit{dissipative} field which is able to model the dissipative components of the damped spring's dynamics. Summing these two fields, we obtain an accurate model of the dynamics of the system. <b>Row 3.</b> We train the baseline model (MLP) on the training set; this model gives a good fit and can be integrated as a Neural ODE, but it cannot be used to decompose the field into conservative and dissipative components. <b>Row 4.</b> We train a Hamiltonian Neural Network (HNN) on the same dataset and find that it is only able to model the conservative component of the system's dynamics. In other words, it strictly enforces conservation of energy in a scenario where energy is not actually conserved.
  </div>
</div>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>D-HNNs build on the foundations of HNNs by bridging the gap between the clean, natural symmetries of physics and the imperfect, messy data often found in the real world. They employ the tools of Hamiltonian mechanics and Helmholtz decomposition to separate conserved quantities from dissipative quantities. In doing so, they allow us to model complex physical systems while enforcing strict conservation laws. They represent a small, practical advance in that they can help us see our datasets, like the ocean currents dataset, in a new way. And at the same time, they represent progress towards the more ambitious goal of building models which make sense of the incredible complexity of the real world by focusing on its invariant quantities.</p>

<h2 id="footnotes">Footnotes</h2>

:ET