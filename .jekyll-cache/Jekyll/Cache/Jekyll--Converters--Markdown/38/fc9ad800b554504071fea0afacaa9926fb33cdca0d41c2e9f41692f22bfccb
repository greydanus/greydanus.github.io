I"º_<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/sparsity/sparsity_wide.png" />
</div>

<h2 id="sculpting-with-sparsity">Sculpting with Sparsity</h2>

<p>Someone once asked Michaelangelo how he carved the statue of David. Was it difficult? ‚ÄúNo,‚Äù Michaelangelo replied, ‚ÄúIt was easy. I just chipped away the stone that didn‚Äôt look like David.‚Äù</p>

<p>When we think of building new things, we tend to think about adding materials together to create something larger and more complex. But this is not always the case. In the case of David, the creative process was one of gradual subtraction. And in much the same way, some of the most interesting models in the field of artificial intelligence are being built by gradually removing specific connections between neurons. Done properly, this pruning produces sparse models that are much smaller, more efficient, and more useful than the originals.</p>

<p>Over the past five years, researchers have managed to train some truly massive AI models. Developing one of these models can cost millions of dollars in electricity alone [ref]. Some of them are so large that they have to be kept on specialized hardware [ref]. The reward for training these models is that they can do some things more effectively than hand-coded computer programs: for example, they can write code, fold proteins, drive Teslas, understand images, and translate languages [refs].</p>

<p>But these models, which researchers are beginning to call, ‚Äúfoundation models,‚Äù [ref] are like great marble edifices. They are powerful and impressive but also unwieldy. For the most part, they are too slow to run on a web server; sometimes they are too large to even fit on a web server [ref]. And they consume more electricity than comparable techniques, to the point of introducing significant long-term costs [ref]. So companies that want to use these models must ask themselves: will the benefits they bring us outweigh the costs of hosting and maintaining them?</p>

<p>For a raw foundation model, that answer is often ‚Äúno.‚Äù But using technology that already exists, we can modify these models so they use 10-1000x less memory, compute, and electricity [ref]. In this new cost regime, the answer to that question often becomes ‚Äúyes.‚Äù Some of this improvement can be accomplished with model distillation, weight quantization, and caching [refs]. But the lion‚Äôs share of this improvement comes from sparsity [refs].</p>

<p>Sparsity is among the most powerful and underrated tools in deep learning. Indeed, there is a good argument that sparsity is the natural language of neural networks.</p>

<h2 id="the-natural-language-of-neural-networks">The Natural Language of Neural Networks</h2>

<p>The largest and most successful models in the field that we loosely refer to as ‚ÄúArtificial Intelligence‚Äù are built from large networks of simulated neurons, often called neural networks. Examples of neural networks have been around ever since computers were invented, but much of the groundwork for their training and evaluation was laid in the 1980‚Äôs and 1990‚Äôs. During that time, prominent researchers like Yann Lecun, Geoffery Hinton, Yoshua Bengio, and Jurgen Schmidhuber often looked at biological neural networks for inspiration [ref].</p>

<p>Let‚Äôs take a moment to look at how the analogy between biological and artificial neural networks works. In order to do this, we‚Äôll begin by focusing our attention on a single neuron in an artificial neural network. The behavior of this neuron can be written as \(f(x \dot w)\), where \(w\) is a vector of N ‚Äúsynaptic weights‚Äù which are real-valued numbers that describe the relative strength of that neuron‚Äôs connections to its neighbors, \(x\) is a vector of equal length which describes the activations of those neighbors, and \(f\) is a scalar function called the ‚Äúactivation function‚Äù which determines when the neuron fires and how strongly. Computing the behavior of the neuron means computing a dot product between two vectors of length N. You might imagine how a group of these neurons, all with the same number of synaptic connections, could be represented as an MxN matrix where each row corresponds to a separate neuron. In a similar way, different sets of activation vectors ‚Äì each corresponding to a different type of stimulus ‚Äì could be stacked atop one another to form a BxN matrix.</p>

<p>[diagram]</p>

<p>In the early days of neural networks, it was rare for researchers to stack more than a few neural network layers atop one another. In these types of ‚Äúshallow‚Äù networks, the total number of synapses grows quadratically with the number of neurons. But one of the things that the researchers noticed was that connections in biological neural networks did not grow in the same way. Consider the human brain, for example. It has around 86 billion neurons [ref] and 150 trillion synapses [ref], implying that only 0.000001% [delete later: (150<em>10^12)/((86</em>10^9)(86<em>10^9-1)/2)=4</em>10^-8] of the possible connections between neurons are actually present. In other words, the connectivity of the brain is 99.999999% sparse. In the brain, the total number of synapses grows linearly with the number of neurons; put another way, each biological neuron gets to have a fixed number of connections, and this number doesn‚Äôt change even as the total size of the network increases.</p>

<p>Not only is the human brain sparse in synaptic connectivity; it is also sparse in neuron activation rates. The energy consumed by a biological neuron is roughly proportional to the number of times it fires [ref]. So the fewer neurons that fire in the brain, the less energy it consumes. By contrast, a simulated neuron like the one we described above consumes the same amount of energy regardless of whether it fires or not. Even if its output is zero, that zero is still multiplied against the weights of the neurons that are wired to it.</p>

<p>Biological neural networks exhibit extreme sparsity in terms of their synaptic connections \(w\) and their activations vectors \(x\). This ‚Äúdouble sparsity‚Äù simplifies the brain‚Äôs wiring patterns and makes it very computationally efficient.</p>

<p>[maybe diagram]</p>

<p>Neural network researchers were aware of this fact and, throughout the 1990‚Äôs, spent great effort looking for different ways to sparsify their models. One approach they used was ‚Äúweight magnitude regularization‚Äù which encouraged synaptic connections to go to zero if they weren‚Äôt improving the model‚Äôs accuracy [ref]. Another approach was ‚Äúmagnitude pruning‚Äù by which synaptic connections were removed entirely if their magnitudes were small [ref]. A related approach, cheerily named ‚ÄúOptimal Brain Damage‚Äù set synapses to zero in proportion to the sizes of their second derivatives (with respect to the model‚Äôs predictions) [ref]. Apart from direct pruning, they spent a great deal of time experimenting with different neural network architectures, like convolutional neural networks, where the number of synaptic connections doesn‚Äôt grow quadratically with the number of neurons. [perhaps add some narrative an analysis here]</p>

<p>We should note that biological analogies were not the only reason that the researchers of the 1990‚Äôs cared about sparsity; they were also working under significant computational constraints. They were training their models on CPUs at the time, and CPUs back then were slow. Neural network research was a niche field that attracted limited funding [give stats, ref]. Most labs were academic and couldn‚Äôt afford large computer systems. And so in consequence, one of the best ways to train models with more neurons was to increase sparsity.</p>

<p>So it was that from the early days, neural network researchers were passionately interested in incorporating sparsity into their models.</p>

<h2 id="an-era-of-extravagance-data-and-compute-in-the-2010s">An Era of Extravagance: Data and Compute in the 2010‚Äôs</h2>

<p>But sparsity as a research field was destined to languish throughout the 2000‚Äôs and early 2010‚Äôs. That it languished in the 2000‚Äôs should come as no surprise: that era is known as the AI Winter. It was a time when funding for neural network research dropped precipitously and all but the most committed researchers shifted their attention to other fields. Progress slowed across all the subfields of neural network research and sparsity was no exception. But the fact that sparsity languished in the early 2010‚Äôs is more surprising. This was a time when the field as a whole was caught up in a massive tidal wave of interest and funding. During this so-called AI Spring, progress occurred at a dizzying pace. Better models and new commercial applications emerged on a monthly basis; a scientific gold rush got underway.</p>

<p>Yet interest in neural network sparsity was lukewarm. How could this be? Researchers knew that biological neural networks were inherently sparse. Moreover, they knew about the compelling computational benefits of using sparse matrices. Even so, most research and commercial applications focused on improving neural networks in other ways. Sparsity was an important consideration, but it rarely occupied center stage.</p>

<p>Perhaps this was because there were so many other fruitful ways for researchers to improve their models. First, since the 1990‚Äôs, the internet had exploded into popular culture and this made it possible for researchers to construct massive text and image datasets using data from the public domain. The ImageNet dataset is a good example. Introduced in 20XX, it consisted of millions of color images from over a thousand categories. Classifying these images was a complex enough task that traditional hand-designed approaches fell short. And when neural network models began to perform well, it became clear that they would soon have valuable real-world applications.</p>

<p>Another fruitful area of research during this time was computing infrastructure. Researchers found that they could massively accelerate training and evaluation of their models by putting them on GPUs. Since GPUs already had a healthy set of commercial applications in gaming and computer graphics, they were relatively cheap and easy to obtain. Moreover, they were designed to process many pieces of data simultaneously and <em>in parallel</em>. As we have seen, running a neural network involves calculating a large number of dot products in parallel, and this makes them a perfect match for GPUs.</p>

<p>A third fruitful means of improving neural networks was the introduction of open-source automatic differentiation (autodiff) frameworks like Theano, TensorFlow, and PyTorch. These libraries brought together code for computing the gradients of neural networks (which is essential for training them, and hard to do by hand), accelerating training with GPUs, and interacting with very large datasets. They allowed a researcher to build and train models using simple code written in a high-level programming language like Python. But underneath the surface, they used cleverly optimized C code and CUDA kernels to make the most of the computer‚Äôs hardware.</p>

<p>The autodiff frameworks also made it easier for reseachers to experiment with new neural network architectures. So it was that in the mid 2010‚Äôs, at the peak of the AI Spring, researchers made great progress in designing better architectures. In 2015, ResNets were discovered and quickly became an industry standard for computer vision. In 2017, Transformer layers were proposed and they soon became the default building block for sequence models. Since much of neural network architecture search is about judiciously choosing how to connect neurons with one another, it is closely related to the topic of sparsity. But while sparsity research solves this problem at the neuron level by setting specific entries in matrices to zero, architecture search addresses it at the layer level by choosing how groups of neurons, represented by layers, are wired to one another.</p>

<p>One of the reasons that architecture search outpaced sparsity methods in the 2010‚Äôs was that researchers were able to make big gains in accuracy and efficiency through architectural changes alone. Autodiff frameworks like TensorFlow made it easy to design new architectures, run them fast on GPUs, and train them on large datasets. And yet, for all of their flexibility in other domains, they were not well suited for sparsity research. One reason is that most ML computing systems (GPUs included) are not optimized for sparse matrix operations. Another reason is that adding full sparse matrix support is a hard technical problem. As long as significant progress was happening in other areas of AI, this was a problem that researchers could put off. But as the 2010‚Äôs drew to a close, questions of energy efficiency and compute-vs-accuracy tradeoffs became more pressing and the unrealized promise of sparsity began to look sweeter.</p>

<h2 id="realizing-the-promise-of-sparsity">Realizing the Promise of Sparsity</h2>

<p>Today‚Äôs neural networks have reached a critical level of proficiency. They are now able to add significant value to a wide range of industries, from electric cars to manufacturing to health care. But the best of these models are also the biggest of these models; there is an inherent tradeoff between their performance and the amount of resources (memory, compute, electricty) they consume. And as time has passed, these models have grown dramatically in size and complexity. Consider, for example, three state of the art neural network models from the past ten years: ResNet (2014) had 0.25 million parameters, BERT (2018) had 3.4 million parameters, GPT-2 and 3 (2019 and 2020) had 1.5 and 175 billion parameters. And as model sizes have grown, so have the datasets they are trained on. The amount of data on the internet more generally is increasing exponentially, especially as 5G and cloud technologies reach maturity. All of these trends point to a growing demand for computational power.</p>

<p>And yet the supply of chips and computing infrastructure is not growing at the same rate. Much of this can be attributed to a deterioration of Moore‚Äôs Law. Moore‚Äôs Law is an empirical observation that the number of transistors on computer chips doubles roughly every two years. Yet this law is beginning to break down because transistors are now so small that they are approaching physical limits on how small they can be etched. Below these limits, quantum effects begin to interfere with their behavior.</p>

<p>As compute becomes more scarce relative to model and dataset size, we can expect neural network researchers to place a higher premium on computational efficiency. In many ways, such a world would feel reminiscent to the early days of AI when computing power was also very limited. In such a world, sparsity will once again take center stage. ‚ÄúThings that deal with sparse parallelism,‚Äù said Raja Koduri, Intel‚Äôs head of chip architecture, ‚Äú‚Ä¶will give rise to some new architectural ideas that are very different from what we are doing in vector-matrix, which is very mainstream right now.‚Äù</p>

<p>There are already a few sigs that this transition towards sparsity is underway. Academic publications referencing sparsity increased markedly in the past five years. Meanwhile, in 2020 NVIDIA released the first chip with a sparsity feature; this chip was called the A100 and it included a ‚Äúsparsity processing unit‚Äù (SPU) with a 2X performance increase. Around the same time, Google researchers published a paper describing initial steps towards adding sparsity support to Tensor Processing Units (‚Äú<a href="https://dl.acm.org/doi/10.1145/3392717.3392751">Sparse-TPU</a>‚Äù). Since then, other companies have taken steps towards supporting sparsity in neural networks. In early 2022, for example, Intel advertised an ‚ÄúIntel Neural Compressor‚Äù tool aimed at helping developers to sparsify their models.</p>

<h2 id="moffett-ai-a-case-study-in-double-sparsity">Moffett AI: a case study in double sparsity</h2>

<p>While these recent developments are moving in the right direction, it is important to put them in context. Researchers have shown that many models can be pruned until they are well over 95% sparse without damaging performance. Naively, this would suggest that such models could be made twenty times smaller and more efficient by adding weight sparsity. And yet existing speedups due to adding sparsity support are closer to a factor of two. Another underexplored area of sparsity research is <em>sparse training</em>. Here, models are initialized with sparse connections and kept at a certain level of sparsity throughout training. This is in contrast to existing sparsity techniques, which generally rely on pruning weights from models <em>after</em> they have been trained. Sparse training is exciting because it will allow researchers to train models that would be <em>too large to fit in computer memory otherwise</em>. For example, if one were to train a 95% sparse model and the weights of that model used the maximum amount of memory available, then we could expect a dense representation of the same model to occupy twenty times the amount of RAM available on that computer!</p>

<p>One of the challenges in adding sparse support is that not all forms of sparsity are the same. There are structured types of sparsity, like block sparsity, where particular regions of matrices are set to zero. There are also structured sparsity <em>distributions</em> wherein connections between neurons are chosen to create small world networks (see Watts-Strogatz and XXX). And of course there are many scenarios where sparsity is unstructured and occurs at a per-synapse level. In some cases, half of a model‚Äôs connections are set to zero (50% sparsity) and in others, nearly all of them are set to zero (99% sparsity). All of these types of sparsity need to be considered and addressed in proportion to how useful and pragmatic they are.</p>

<p>So far, companies have made incremental modifications to existing chip architectures. For example, the speedups offered by NVIDIA‚Äôs A100 chip come from making a large sparse matrix look like a small dense matrix. Doing this can produe a 2x acceleration, but it cannot unlock the order of magnitude gains which, in theory, sparsity offers.</p>

<p>Progress has been slow in this area because sparsity is a difficult hardware problem. Adding full sparsity support means representing matrices and vectors differently on hardware. It means structuring the implementations of dot products differently. It means parallelizing computations in different ways. There is a growing consensus that, in order to accomodate these changes, AI chips need to be rebuilt from the ground up.</p>

<p>This task, which requires daring and flexibility, is well suited for startups. Indeed, some of the best work being done on sparsity hardware is being done by small companies. Numenta, a Bay Area startup, recently demonstrated a custom chip with hardware support that runs a popular vision architecture XX% faster than more traditional chips. Another company, NeuralMagic, focuses on model sparsification specifically for CPUs, so as to allow foundation models to run on consumer-grade computers and laptops.</p>

<p>Among the most interesting and ambitious companies working on sparsity is Moffett AI. Moffett is working on ‚Äúdouble sparse,‚Äù which refers to sparsity support for both the weights and the activations in a neural network. The image below gives an intuitive comparison of the differences between dense-dense operations, which the majority of AI chips use currently, dense-sparse operations, which some chips like the NVIDIA A100 offer, and ‚Äúdouble sparse‚Äù operations which Moffett supports. One thing to notice is that using double sparsity permits researchers to evaluate the products of much larger matrices while using the same amount of memory, compute, and energy. In the image above, this leads to efficiency gains of one two two orders of magnitude. In other words, Moffett‚Äôs appraoch permits researchers to train and evalute models that are 10-100x larger for the same cost.</p>

<p>The practical benefits of double sparsity are fourfold: they include increased speed, higher accuracy, less energy consumption and lower cost. To make these benefits more concrete, let‚Äôs compare Moffett‚Äôs latest double sparse chip, the ANTOUM, to its dense-dense and dense-sparse counterparts. The first thing to notice is that can run sparse models 32x and 8x faster than the two respective baselines. These gains in speed are important because model latency matters a great deal in real-world settings. For example, a self-driving car needs to be able to process frames at at least X fps in order to avoid obstacles while moving at X mph. As another example, internet users begin to lose interest in a webpage if its latency grows beyond a few dozen milliseconds, meaning that any AI models running in the background need to be able to keep up. The second benefit of double sparse is that it permits higher-accuracy models to run for the same computational budget. In other words, since we‚Äôre able to run models 8-32x faster, we can fit 8-32x larger models on the same chips and thus see a substantial boost in accuracy.</p>

<p>A similar line of reasoning is behind that third benefit, which is less energy consumption. The energy that a chip consumes is roughly proportionate to the number of mathematical operations it performs. Since double sparse hardware ignores all the activations and weights that are set to zero, it saves the energy that would have been used to multiply them with other numbers, only to get more zeros. In the same way that this property leads to an 8-32x speedup, it also leads to an 8-32x energy savings. Since the cost of running an AI chip in a datacenter is roughly proportional to the cost of the electricity it uses, we can deduce that the ANTOUM provides a fourth notable benefit: an 8-32x reduction in operating expenses.</p>

<p>These</p>

<p>Double sparsity gives ANTOUM some considerable advantages, but we should note that ANTOUM is just the first step</p>

<p>The ANTOUM is interesting because it showcases the advantages of using double sparsity</p>

<p>As we saw earlier, in the context of biological systems, double sparsity offers dramatic improvements in efficiency compared to dense models, or even models with sparse weights and dense activations. Yet double sparsity comes with some difficult technical challenges. In particular, it requires that hardware and software be developed in tandem, by the same team of researchers. This is what Moffett AI has set out to accomplish.</p>

<p>Moffett is a Series A startup founded in 2018 with the goal of ‚Äúbuilding the world‚Äôs next-generation AI computation platform, from the ground up.‚Äù One of the core themes of the company is that sparsity is both a hardware and software design problem. And these are not two separate problems. In other words, the right sparse chip design depends on how the software running on it is written. Conversely, the best software for sparse AI depends on what hardware is being used. This theme is reflected in the company‚Äôs four founders: two have backgrounds in chip design, one has a background in ML software, and one has a mix of hardware and software experience.</p>

<p>[diagram]</p>

<p>The figure above gives</p>

<p>(paragraph: double sparsity in particular)</p>

<p>(paragraph: hardware/software codesign in particular)</p>

<p>(paragraph: the context of the industry and how Moffett is positioned to do quite well)</p>

<h2 id="frontiers-of-sparsity">Frontiers of sparsity</h2>

<p>(this is a conclusion section: zoom back out, return to the theme of ‚Äúsculpting with sparsity‚Äù and comment on how it has been partially realized but how there is much yet to be chiseled away. In coming years we will have adapted everything from hardware to low-level kernels to high-level autodiff frameworks in order to accomodate sparsity ‚Äì as this happens, we will slim down the large blocky models of today and reveal them to be elegant and efficient solutions to the computing problems of tomorrow)</p>

<p>(this will maybe be two paragraphs total)</p>
:ET