I"4<p>Let’s imagine we were going to build a simulation of the universe. Starting from what we know about physics, how would we inmplement it? I wrote down a few ideas. It would be:</p>

<ol>
  <li><strong>Massively parallel.</strong> By taking advantage of the fact that all interactions are local and causal interactions cannot propagate faster than the speed of light, one could parallelize the simulation in a dramatic way. Spatially adjacent parts of the simulation would be run on the same CPUs and spatially distant parts would almost certainly be run on separate CPUs.</li>
  <li><strong>Conservation laws enforced.</strong> Physics is built on the idea that certain quantities are strictly conserved. Scalar quantities like mass-energy are conserved. So are vector quantities like spin and angular momentum.</li>
  <li><strong>Binary logic.</strong> Our computers use discrete, binary logic to represent and manipulate information. In this world, non-discrete numbers (eg. floats) are represented with sequences of discrete symbols. Let’s assume that a simulation of the universe would do the same thing.</li>
  <li><strong>Adaptive computation.</strong> A lot more happens in some times and places in the universe than others. To simulate it efficiently, we would not want to make a 3D grid and then put mass in each of the cells of that grid – 99% of the grid would be empty. Instead, we’d want to use a particle-based (Lagrangian) simulation where the distribution of matter and energy is represented as nodes on a graph.</li>
</ol>

<p>We can check to see whether these are good assumptions by looking at whether they hold true for existing state-of-the-art physics simulations. Taking a quick glance over the best <a href="https://www.myroms.org/">oceanography</a>, <a href="https://confluence.ecmwf.int/display/S2S/ECMWF+model+description">meteorology</a>, <a href="https://arxiv.org/abs/0810.5757">plasma</a>, and <a href="https://en.wikipedia.org/wiki/Computational_fluid_dynamics">computational fluid dynamics</a> models, this appear to be the case.</p>

<p>Now, having laid out some basic assumptions about our simulation of the universe, let’s look at their implications.</p>

<p>The first thing to see is that assumptions 1 and 2 are in tension with one another. In order to ensure that a quantity (eg. mass or energy or mass-energy) is conserved, you need to sum that quantity across the entire simulation, determine whether a correction is needed, and then apply that correction to the system as a whole. Computationally, this requires a synchronous reduction operation and an element-wise divide across the whole system, and this has to be done at virtually every simulation step. When you write a single-threaded physics simulation, this can account for about half of the computational cost (here are examples from the simple <a href="https://github.com/greydanus/optimize_wing/blob/3d661cae6ca6a320981fd5fc29848e1233d891cd/simulate.py#L57">fluid</a> and <a href="https://github.com/google-research/neural-structural-optimization/blob/1c11b8c6ef50274802a84cf1a244735c3ed9394d/neural_structural_optimization/topo_physics.py#L236">topology</a> simulations I have written).</p>

<p>As you parallelize the simulation, you can expect the cost of enforcing conservation laws to grow much higher. This is because simulating interactions between adjacent particles is pretty easy to do in a parallel and asynchronous manner. But enforcing system-wide conservation laws requires transferring data between distant CPU cores and keeping them more or less in sync. As a result, enforcing conservation laws in this manner quickly grows to be a limiting factor on runtime.</p>

<p>We may ask if perhaps there is some other way to enforce conservation laws without tallying conserved quantities across the entire simulation at each step. Probably yes! One way to do this is to quantize the conserved quantity. For example, we could quantize energy and then only transfer it in the form of discrete packets. Let’s take financial markets as an analogy. They are massively parallel and it’s important to keep a proper accounting of the total amount of currency in circulation at a given moment. The solution is to allow currency to function as a continuous value on a practical level, but to quantize it by making small measures of value (pennies) indivisible.</p>

<p>Quantization may work well for conserving scalar values like energy. But what about conserving vector quantities like angular momentum? In these cases, rotational symmetry makes things difficult. Imagine two particles with angular momentum vectors pointing in arbitrary directions. When we compute the angular momentum of the entire system, odds are we will get an irrational number. There is simply no way to quantize the directions of the two particles’ angular momentum vectors so that the sum of the two is also quantized without sacrificing rotational symmetry.</p>

<p>So how are we to implement exact conservation of vector quantities without sacrificing rotational symmetry? One option is to require that a particle’s vector quantities always be defined win reference to some other particle’s vector quantities. This would be implemented by creating multiple <a href="https://en.wikipedia.org/wiki/Pointer_(computer_programming)">pointer references</a> to a single variable and then giving each of those pointers to a different particle. As a concrete example, we might imagine an atom releasing energy in the form of two photons. The polarization angle of the first photon would be expressed as a pointer to variable <code class="language-plaintext highlighter-rouge">x</code> whereas that of the second photon would be expressed as a 90 degree rotation of a pointer to variable <code class="language-plaintext highlighter-rouge">x</code>. Moving our simulation forward, the polarization angles of the two phorons will change over time and perhaps some small amount of integration error will accumualte. But even if some integration error occurs, we can say with confidence that the net polarization of the two photons is zero because they are defined in terms of the same latent variable.</p>

<p>This approach is a good way to enforce exact conservation of vector quantities. However, it comes at a price. We must sacrifice locality – the principle that an object is influenced directly only by its immediate surroundings. It’s one of the most sacred principles in physics. This gets violated in the example of the two photons because a change in the polarization of the first photon will update the value of <code class="language-plaintext highlighter-rouge">x</code>, which will implicitly change the polarization of the second photon.</p>

<p>Interestingly, the mechanics of this nonlocal relationship would predict the violation of <a href="https://www.youtube.com/watch?v=zcqZHYo7ONs&amp;vl=en">Bell’s inequality</a>.</p>

<p>Physicists agree violation of Bell’s inequality implies that nature violates either realism (the principle that reality exists with definite properties even when not being observed) or locality. Since locality is seen as a more fundamental principle than realism, the modern consensus is that quantum mechanics violates realism: in other words, entangled particles cannot be said to have deterministic states and exist in a state of superposition until their are measured. As a contrast, in our simulated universe, realism would be preserved whereas exact locality would be sacrificed. Entangled particles could be said to have a definite state at all times, but whenever one particle interacted with some piece of matter, that interaction would affect the state of the other particle instantaneously.</p>

<p>This observation may lead us to ask whether other quantum mechanical phenomena are byproducts of simulation. The double slit experiment is an interesting example. We cannot explain a photon’s “self-interaction” using the argument we made above about conservation of vector quantities. Why, then, might particles exhibit wave-particle duality in a simulation?</p>

<p>The probable answer to this is filtering. Filtering is a common technique where a Gaussian (or cone) filter is convolved with a grid in order to smooth out the physics on the grid and make them appear as though they were occuring on a continuous medium. This step is absolutely essential when one wants to simulate continous physics on a discrete domain. The filtering steps in the <a href="https://github.com/greydanus/optimize_wing/blob/3d661cae6ca6a320981fd5fc29848e1233d891cd/simulate.py#L83">fluid</a> and <a href="https://github.com/google-research/neural-structural-optimization/blob/1c11b8c6ef50274802a84cf1a244735c3ed9394d/neural_structural_optimization/topo_physics.py#L84">topology</a> simulations I referred to earlier were just as essential as enforcing conservation laws. Moreover, filtering is used in most large-scale simulations like the ones we discussed at the beginning of the post.</p>

<p>quantized angular momentum vector, \(\theta\). Now, imagine that a new particle with the minimum amount of angular momentum is added to the group. In order for this operation to be a success, the particle’s angular momentum vector cannot be pointed in some arbitrary direction. If it were, then the new angular momentum of the group of particles would</p>

<p>In order to do so, we would need to quantize the parameterization of space itself. In other words, we would have to make some spin or polarization angles off limits.</p>

<p>This is how the financial system  money functions on a practical level as a continuous quantity, but</p>

<p>We can take advantage of the fact that interactions between particles in our simulation are entirely local.</p>

<p>The idea is to write a globally conserved quantity as a sum of locally-conserved quantities over a particular timeframe (this assumes that there will be very little transfer of the conserved quantity between each of the local regions. Note: physicists do this all the time in lab experiments). Individual particles or nodes in the simulation will have pointers that reference these locally-conserved quantities. Then, as these particles interact with their neighbors, they will gain new (weighted) pointers that point to the locally-conserved quantities of their neighbors. For example, a particle might transfer energy to its neighbor by passing it a pointer reference to its energy variable along with a coefficient describing how much of that energy is being transferred. A useful analogy to this would be a person giving their neighbor a paper banknote describing fractional rights to a certain amount of gold stored somewhere.</p>

<p>In cases where one of these assumptions doesn’t hold, it’s usually because they make the implementation more technically challenging; for example, it’s easier to implement a simuation on one CPU than it is to distribute it across many CPUs. And it’s easier to use a grid parameterization than it is to use a mesh (graph) parameterization. But the largest and most accurate simulations</p>

<p>Let’s imagine the universe is being simulated. Based on what we know about physics, what can we say about how the simulation would be implemented? Here are a few ideas
Massively parallel. By taking advantage of the fact that all interactions are local, one could parallelize the simulation in a dramatic way. Spatially adjacent parts of the simulation would be run on the same CPUs or on adjacent CPUs
Conservation laws enforced. Physics is built on the idea that certain quantities are strictly conserved. Scalar quantities like mass-energy are conserved. And vectors like spin and angular momentum are conserved.
Binary logic. Our computers use discrete logic – more specifically, the binary logic of (0’s and 1’s) – to represent everything. Continuous values like floats are represented via a series of binary values. Let’s assume that a simulation of the universe would do the same thing.
Same code across time and space. As far as we can tell, the laws of physics remain invariant across space and time. The simplest assumption is that the code implementing those laws does the same thing.</p>

<p>Adaptive computation. A lot more is happening in some areas of the universe than others. So to simulate it efficiently, one would probably not parameterize it with a volumetric 3D grid. Instead, one would probably represent the state of the universe as a mesh or a graph where the interacting nodes are at different distances from one another. If you look at a piece of matter over time, you’ll notice that sometimes more is happening (eg a supernova occurs, or life on earth appears) and other times less is happening (a cold asteroid simply exists for a million years)
We can check to see whether these are good assumptions by looking at whether they hold true for current state of the art physics simulations. For the most part, they do: most computational physics simulations are massively parallel, they take great care to enforce conservation laws (a step sometimes called “projection” in computational fluid dynamics), and they tend to use adaptive computation across time and space (via adaptive ODE solvers across time and mesh parameterizations across space).</p>

:ET