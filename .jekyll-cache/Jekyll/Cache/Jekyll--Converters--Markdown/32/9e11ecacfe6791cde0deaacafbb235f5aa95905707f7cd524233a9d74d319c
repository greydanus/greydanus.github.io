I"†1<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hero.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  Dissipative HNNs (D-HNNs) output two scalar functions, denoted here by <i><b>H</b></i> and <i><b>D</b></i>. The first of these two, <i><b>H</b></i>, is the Hamiltonian. We use its symplectic gradient to model energy-conserving dynamics. The second, <i><b>D</b></i>, is the Rayleigh dissipation function. It models the dissipative component of the dynamics of a physical system. The addition of the dissipation function is what sets this model apart from Hamiltonian Neural Networks; it allows D-HNNs to model systems where energy is not quite conserved â€“ as, for example, in the case of a damped mass-spring system.
  </div>
</div>

<div style="display: block; margin-left: auto; margin-right:auto; width:100%; text-align:center;">
  <a href="https://arxiv.org/abs/2201.10085" id="linkbutton" target="_blank">Read the paper</a>
  <a href="https://github.com/greydanus/dissipative_hnns" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="a-sea-of-change">A sea of change</h2>

<p>We are immersed in a complex, dynamic world where change is the only constant. And yet there are certain patterns to this change that suggest natural laws. These laws include conservation of mass, energy, and momentum. Taken together, they constitute a powerful simplifying constraint on reality. Indeed, physics tells us that a small set of laws and their associated invariances are at the heart of all natural phenomena. Whether we are studying weather, ocean currents, earthquakes, or molecular interactions, we should take care to respect these laws. And when we apply learning algorithms to these domains, we should ensure that they, too, respect these laws.</p>

<p>We can do this by building models that are primed to learn invariant quantities from data: these models include HNNs, LNNs, and a growing class of related models. But one problem with this models is that, for the most part, they can only handle data where some quantity (such as energy) is exactly conserved. If the data is collected in the real world, and there is even a small amount of friction, then these models will struggle. In this post, weâ€™ll indroduce Dissipative HNNs, a class of models which can learn conservation laws from data even when energy isnâ€™t perfectly conserved.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/sea_of_change.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  We live in a sea of change. But no matter how complex a system's dynamics are, they can always be decomposed into the sum of dissipative dynamics and conservative dynamics.
  </div>
</div>

<p>The core idea is to parameterize both a Hamiltonian <em>and</em> a Rayleigh dissipation function. During training, the Hamiltonian function fits the conservative (rotational) component of the dynamics whereas the Rayleigh function fits the dissipative (irrotational) component.</p>

<h2 id="a-quick-theory-section">A quick theory section</h2>

<p><strong>The Hamiltonian function.</strong> The Hamiltonian \(\mathcal{H}(\textbf{q},\textbf{p})\) is scalar function where by definition \( \frac{\partial \mathcal{H}}{\partial \textbf{p}} = \frac{\partial \textbf{q}}{dt},  -\frac{\partial \mathcal{H}}{\partial \textbf{q}} = \frac{\partial \textbf{p}}{dt} \). This constraint tells us that, even as the position and momentum coordinates of the system \(\textbf{(q, p)}\) change, the scalar output \(\mathcal{H}\) remains fixed. In other words, \(\mathcal{H}\) is invariant with respect to \(\textbf{q}\) and \(\textbf{p}\) as they change over time; it is a conserved quantity. Hamiltonians often appear often in physics because, for every natural symmetry (or law), there is a corresponding conserved quantity (see <a href="https://en.wikipedia.org/wiki/Noether%27s_theorem">Noetherâ€™s theorem</a>).</p>

<p><strong>The Rayleigh function.</strong> The Rayleigh dissipation function \(\mathcal{D}(\textbf{q},\textbf{p})\) is a scalar function that provides a way to account for dissipative forces such as friction in the context of Hamiltonian mechanics. As an example, the Rayleigh function for linear, velocity-dependent dissipation would be \(\mathcal{D} = \frac{1}{2}\rho\dot{q}^2\) where \(\rho\) is a constant and \(\dot q\) is the velocity coordinate. We add this function to a Hamiltonian whenever the conserved quantity we are trying to model is changing due to sources and sinks. For example, if \(\mathcal{H}\) measures the total energy of a damped mass-spring system, then we could add the \(\mathcal{D}\) we wrote down above to account for the change in total energy due to friction.</p>

<p><strong>Helmholtz decompositions.</strong> Like many students today, Hermann von Helmholtz realized that medicine was not his true calling. Luckily for us, he pursued physics instead and discovered one of the most useful tools in vector analysis: the Helmholtz decomposition. The Helmholtz decomposition says that any vector field \(V\) can be written as the gradient of a scalar potential \(\phi\) plus the curl of a vector potential \(\mathcal{\textbf{A}}\). In other words, \( V = \nabla\phi + \nabla\times \mathcal{\textbf{A}}\). Note that the first term is irrotational and the second term is rotational: <em>any vector field can be decomposed into the sum of an irrotational (dissipative) vector field and a rotational (conservative) vector field</em>. Hereâ€™s a visual example of a Helmholtz decomposition (taken from <a href="https://drive.google.com/file/d/1upKFdtnM0xcTVxNsPHI1KCvmcanAJheL/view?usp=sharing">this paper</a>):</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hhd.jpg" />
</div>

<p><strong>Putting it together.</strong> In the Hamiltonian Neural Networks (HNN) post and paper, we showed how to parameterize the Hamiltonian function and then learn it directly from data. Here, we parameterize a Rayleigh function as well. Our model looks the same as an HNN except now it has a second scalar output for the Rayleigh function, as shown in the first figure of this post. During the forward pass of this model, we take the symplectic gradient of the Hamiltonian to obtain conservative forces; the important thing to notice is that the symplectic gradient gives us a rotational vector field over the domain of the modelâ€™s inputs. Meanwhile, we take the gradient of the Rayleigh function to obtain dissipative forces; notice that the gradient gives us an irrotational vector field over the same domain. So, when we fit our model to some vector field, it learns an implicit Helmholtz decomposition of that vector field!</p>

<h2 id="an-introductory-model">An introductory model</h2>

<p>We coded up a D-HNN model and used it to fit datasets from three different physical systems: a synthetic damped mass-spring system, a real-world pendulum system, and a timeseries observation of ocean currents taken from the OSCAR dataset. We will begin with the synthetic damped mass-spring system in order to gain some basic intuitions for how D-HNNs work.</p>

<p>The damped mass-spring system is one dimensional and we can describe it with just two coordinates, \(q\) and \(p\). We can plot them on \(x\) and \(y\) axes respectively to obtain <a href="https://en.wikipedia.org/wiki/Phase_space">phase-space diagrams</a>. These diagrams are useful because, once we fit our model and a few baselines to this dataset, we plotted their predictions in phase space in order to compare them to each other, and to a ground-truth trajectory.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/dampedspring.jpg" style="width:100%; max-width: 600px" />
</div>

<p>In the image above, you can find the entire damped mass-spring dataset plotted in the upper left square. Each arrow is located at a \((p,q)\) coordinate which is also the input to our model. The direction and magnitude of the arrow represents the corresponding time derivative of those coordinates. Obviously, this plot is a vector field. The Helmholtz decomposition tells us that this vector field can be decomposed into conservative and dissipative components, and indeed that is what we have done in the second and third columns (note: in practice, we performed this decomposition using a few hundred iterations of the Gauss-Seidel method to solve Poissonâ€™s equation. Again, see <a href="https://drive.google.com/file/d/1upKFdtnM0xcTVxNsPHI1KCvmcanAJheL/view?usp=sharing">this paper</a> for details.)</p>

<p>Now, in the second row, we train a D-HNN on this dataset. It produces a trajectory that closely matches the ground truth. By plotting the symplectic gradient of \(\mathcal{H}\) and the gradient of \(\mathcal{D}\), we can see that it has properly decoupled the conservative and dissipative dynamics respectively. By contrast, in the third row, we train a baseline model (an MLP) on the same data; this model produces a good trajectory but is unable to learn conservative and dissipative dynamics separately (the second and third columns are thus left blank). Finally, in the fourth row, we train an HNN on the same dataset and find that it is only able to model the conservative component of the systemâ€™s dynamics. It strictly enforces conservation of energy in a scenario where energy is not actually conserved, leading to a poor trajectory prediction.</p>

<h2 id="why-decouple-conservative-and-dissipative-dynamics">Why decouple conservative and dissipative dynamics?</h2>

<p>Weâ€™ve described a model that can learn conservative and dissipative dynamics separately and shown that it works on a toy problem. Why is this a good idea? One answer is that <em>it lets our model fit data in a more physically-realistic manner, leading to better generalization</em>.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/spring_rho.jpg" style="width:100%; max-width: 600px" />
</div>

<p>In the toy example above, we our dataset was constructed using a single friction coefficient, \(\rho\). The MLP we trained gives good predictions for this friction coefficient, but it would be useless if we wanted to predict the massâ€™s trajectory if friction were halved. However, since our D-HNN models friction as a dissipative force, we can do exactly that by multiplying its dissipative vector field by a factor of two or one half. The image above shows that it can produce physically plausible trajectories for these new friction coefficients (highlighted in orange) <em>even though it was only trained on data with one friction coefficient</em>.</p>

<h2 id="additional-experiments">Additional experiments</h2>

<p>We also trained our model on data from a real pendulum and ocean current data from the OSCAR dataset (a visualization of our modelâ€™s fit on the ocean dataset is shown above). On these larger and more more difficult tasks, our model continuted to decouple conservative and dissipative dynamics. The details and results are outside the scope of this post, though. If you are interested, you should read <a href="https://github.com/greydanus/dissipative_hnns">the full paper</a>.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/ocean.jpg" style="width:100%" />
</div>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>D-HNNs build on the foundations of HNNs by bridging the gap between the clean, natural symmetries of physics and the imperfect, messy data often found in the real world. They employ the tools of Hamiltonian mechanics and Helmholtz decomposition to separate conserved quantities from dissipative quantities. In doing so, they allow us to model complex physical systems while enforcing strict conservation laws. They represent a small, practical advance in that they can help us see our datasets, like the ocean currents dataset, in a new way. And at the same time, they represent progress towards the more ambitious goal of building models which make sense of the incredible complexity of the real world by focusing on its invariant quantities.</p>

<h2 id="footnotes">Footnotes</h2>

:ET