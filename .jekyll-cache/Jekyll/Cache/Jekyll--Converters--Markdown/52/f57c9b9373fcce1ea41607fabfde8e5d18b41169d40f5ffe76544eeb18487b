I"’<p>Let‚Äôs imagine we were going to build a simulation of the universe. Starting from what we know about physics, how would we inmplement it? I wrote down a few ideas. It would be:</p>

<ol>
  <li><strong>Massively parallel.</strong> By taking advantage of the fact that all interactions are local and causal interactions cannot propagate faster than the speed of light, one could parallelize the simulation in a dramatic way. Spatially adjacent parts of the simulation would be run on the same CPUs and spatially distant parts would almost certainly be run on separate CPUs.</li>
  <li><strong>Conservation laws enforced.</strong> Physics is built on the idea that certain quantities are strictly conserved. Scalar quantities like mass-energy are conserved. So are vector quantities like spin and angular momentum.</li>
  <li><strong>Binary logic.</strong> Our computers use discrete, binary logic to represent and manipulate information. In this world, non-discrete numbers (eg. floats) are represented with sequences of discrete symbols. Let‚Äôs assume that a simulation of the universe would do the same thing.</li>
  <li><strong>Adaptive computation.</strong> A lot more happens in some times and places in the universe than others. To simulate it efficiently, we would not want to make a 3D grid and then put mass in each of the cells of that grid ‚Äì 99% of the grid would be empty. Instead, we‚Äôd want to use a particle-based (Lagrangian) simulation where the distribution of matter and energy is represented as nodes on a graph.</li>
</ol>

<p>We can check to see whether these are good assumptions by looking at whether they hold true for existing state-of-the-art physics simulations. Taking a quick glance over the best <a href="https://www.myroms.org/">oceanography</a>, <a href="https://confluence.ecmwf.int/display/S2S/ECMWF+model+description">meteorology</a>, <a href="https://arxiv.org/abs/0810.5757">plasma</a>, and <a href="https://en.wikipedia.org/wiki/Computational_fluid_dynamics">computational fluid dynamics</a> models, this appear to be the case.</p>

<p>Now, having laid out some basic assumptions about our simulation of the universe, let‚Äôs look at their implications.</p>

<p>The first thing to see is that assumptions 1 and 2 are in tension with one another. In order to ensure that a quantity (eg. mass or energy or mass-energy) is conserved, you need to sum that quantity across the entire simulation, determine whether a correction is needed, and then apply that correction to the system as a whole. Computationally, this requires a synchronous reduction operation and an element-wise divide across the whole system, and this has to be done at virtually every simulation step. When you write a single-threaded physics simulation, this can account for about half of the computational cost (here are examples from the simple <a href="https://github.com/greydanus/optimize_wing/blob/3d661cae6ca6a320981fd5fc29848e1233d891cd/simulate.py#L57">fluid</a> and <a href="https://github.com/google-research/neural-structural-optimization/blob/1c11b8c6ef50274802a84cf1a244735c3ed9394d/neural_structural_optimization/topo_physics.py#L236">topology</a> simulations I have written).</p>

<p>As you parallelize the simulation, you can expect the cost of enforcing conservation laws to grow much higher. This is because simulating interactions between adjacent particles is pretty easy to do in a parallel and asynchronous manner. But enforcing system-wide conservation laws requires transferring data between distant CPU cores and keeping them more or less in sync. As a result, enforcing conservation laws in this manner quickly grows to be a limiting factor on runtime.</p>

<p>We may ask if perhaps there is some other way to enforce conservation laws without tallying conserved quantities across the entire simulation at each step. Probably yes! One way to do this is to quantize the conserved quantity. For example, we could quantize energy and then only transfer it in the form of discrete packets. Let‚Äôs take financial markets as an analogy. They are massively parallel and it‚Äôs important to keep a proper accounting of the total amount of currency in circulation at a given moment. The solution is to allow currency to function as a continuous value on a practical level, but to quantize it by making small measures of value (pennies) indivisible.</p>

<p>This is how the financial system  money functions on a practical level as a continuous quantity, but</p>

<p>We can take advantage of the fact that interactions between particles in our simulation are entirely local.</p>

<p>The idea is to write a globally conserved quantity as a sum of locally-conserved quantities over a particular timeframe (this assumes that there will be very little transfer of the conserved quantity between each of the local regions. Note: physicists do this all the time in lab experiments). Individual particles or nodes in the simulation will have pointers that reference these locally-conserved quantities. Then, as these particles interact with their neighbors, they will gain new (weighted) pointers that point to the locally-conserved quantities of their neighbors. For example, a particle might transfer energy to its neighbor by passing it a pointer reference to its energy variable along with a coefficient describing how much of that energy is being transferred. A useful analogy to this would be a person giving their neighbor a paper banknote describing fractional rights to a certain amount of gold stored somewhere.</p>

<p>In cases where one of these assumptions doesn‚Äôt hold, it‚Äôs usually because they make the implementation more technically challenging; for example, it‚Äôs easier to implement a simuation on one CPU than it is to distribute it across many CPUs. And it‚Äôs easier to use a grid parameterization than it is to use a mesh (graph) parameterization. But the largest and most accurate simulations</p>

<p>Let‚Äôs imagine the universe is being simulated. Based on what we know about physics, what can we say about how the simulation would be implemented? Here are a few ideas
Massively parallel. By taking advantage of the fact that all interactions are local, one could parallelize the simulation in a dramatic way. Spatially adjacent parts of the simulation would be run on the same CPUs or on adjacent CPUs
Conservation laws enforced. Physics is built on the idea that certain quantities are strictly conserved. Scalar quantities like mass-energy are conserved. And vectors like spin and angular momentum are conserved.
Binary logic. Our computers use discrete logic ‚Äì more specifically, the binary logic of (0‚Äôs and 1‚Äôs) ‚Äì to represent everything. Continuous values like floats are represented via a series of binary values. Let‚Äôs assume that a simulation of the universe would do the same thing.
Same code across time and space. As far as we can tell, the laws of physics remain invariant across space and time. The simplest assumption is that the code implementing those laws does the same thing.</p>

<p>Adaptive computation. A lot more is happening in some areas of the universe than others. So to simulate it efficiently, one would probably not parameterize it with a volumetric 3D grid. Instead, one would probably represent the state of the universe as a mesh or a graph where the interacting nodes are at different distances from one another. If you look at a piece of matter over time, you‚Äôll notice that sometimes more is happening (eg a supernova occurs, or life on earth appears) and other times less is happening (a cold asteroid simply exists for a million years)
We can check to see whether these are good assumptions by looking at whether they hold true for current state of the art physics simulations. For the most part, they do: most computational physics simulations are massively parallel, they take great care to enforce conservation laws (a step sometimes called ‚Äúprojection‚Äù in computational fluid dynamics), and they tend to use adaptive computation across time and space (via adaptive ODE solvers across time and mesh parameterizations across space).</p>

:ET