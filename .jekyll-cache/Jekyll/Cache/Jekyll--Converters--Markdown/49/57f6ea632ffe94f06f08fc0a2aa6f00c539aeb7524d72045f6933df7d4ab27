I"»%<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/sparsity/sparsity_wide.png" />
</div>

<h2 id="sculpting-with-sparsity">Sculpting with Sparsity</h2>

<p>Someone once asked Michaelangelo how he carved the statue of David. Was it difficult? ‚ÄúNo,‚Äù Michaelangelo replied, ‚ÄúIt was easy. I just chipped away the stone that didn‚Äôt look like David.‚Äù</p>

<p>When we think of building new things, we tend to think about adding materials together to create something larger and more complex. But this is not always the case. In the case of David, the creative process was one of gradual subtraction. And in much the same way, some of the most interesting models in the field of artificial intelligence are being built by gradually removing specific connections between neurons. Done properly, this pruning produces sparse models that are much smaller, more efficient, and more useful than the originals.</p>

<p>Over the past five years, researchers have managed to train some truly massive AI models. Developing one of these models can cost millions of dollars in electricity alone [ref]. Some of them are so large that they have to be kept on specialized hardware [ref]. The reward for training these models is that they can do some things more effectively than hand-coded computer programs, such as writing code, folding proteins, driving Teslas, understanding images, generating long, coherent stories, playing complex strategy games, translating languages, forecasting the weather, and generating works of art [refs].</p>

<p>But these models, which researchers are beginning to call, ‚Äúfoundation models,‚Äù [ref] are like great marble edifices. They are powerful and impressive but also unwieldy. For the most part, they are too slow to run on a web server; sometimes they are too large to even fit on a web server [ref]. They consume more electricity than comparable techniques ‚Äì to the point of introducing significant long-term costs [ref]. So companies that want to use these models must ask themselves: will the benefits they bring us outweigh the costs of hosting and maintaining them?</p>

<p>For a raw foundation model, that answer is often ‚Äúno.‚Äù But using technology that already exists, we can modify these models so they use 10-1000x less memory, compute, and electricity [ref]. In this new cost regime, the answer to that question often becomes ‚Äúyes.‚Äù Some of this improvement can be accomplished with model distillation, weight quantization, and caching [refs]. But the lion‚Äôs share of that improvement ‚Äì and a likely source of future improvements ‚Äì is sparsity [refs].</p>

<p>Sparsity is among the most powerful and underrated tools in deep learning. Indeed, there is a good argument that sparsity is the natural language of neural networks.</p>

<h2 id="the-natural-language-of-neural-networks">The Natural Language of Neural Networks</h2>

<p>The largest and most successful models in the field that we loosely refer to as ‚ÄúArtificial Intelligence‚Äù are built from large networks of simulated neurons, often called neural networks. Examples of neural networks have been around ever since computers were invented, but much of the groundwork for their training and evaluation was laid in the 1980‚Äôs and 1990‚Äôs. During that time, prominent researchers like Yann Lecun, Geoffery Hinton, Yoshua Bengio, and Jurgen Schmidhuber often looked at biological neural networks for inspiration [ref].</p>

<p>As an example, let us consider a single neuron in an artificial neural network. The behavior of this neuron can be written as f(x \dot w), where \(w\) is a vector of N ‚Äúsynaptic weights‚Äù which are real-valuated numbers that describe the relative strength of that neuron‚Äôs connections to its neighbors, \(x\) is a vector of equal length which describes the activations of those neighbors, and \(f\) is a scalar function called the ‚Äúactivation function‚Äù which determines when the neuron fires and how strongly. Computing the behavior of the neuron means computing a dot product between two vectors of length N. You might imagine how a group of these neurons, all with the same number of synaptic connections, could be represented as an MxN matrix where each row corresponds to a separate neuron. In a similar way, different sets of activation vectors ‚Äì each corresponding to a different type of stimulus ‚Äì could be stacked atop one another to form a BxN matrix.</p>

<p>[diagram]</p>

<p>In the early days of neural networks, it was rare for researchers to stack more than a few neural network layers atop one another. In these types of ‚Äúshallow‚Äù networks, the total number of synapses grew quadratically with the number of neurons. But one of the things that the researchers noticed was that connections in biological neural networks did not scale the same way. Consider the human brain. It has around 86 billion neurons [ref] and 150 trillion synapses [ref], implying that only 0.000001% [delete later: (150<em>10^12)/((86</em>10^9)(86<em>10^9-1)/2)=4</em>10^-8] of the possible connections between neurons are actually present. In other words, the connectivity of the brain is 99.999999% sparse. The reason for this extreme sparsity is that the number of synapses in the brain grows linearly with the number of neurons. So while the computation per neuron in artificial networks grew in proportion to the number of neurons, it was known to remain constant in biological systems as a consequence of sparsity.</p>

<p>Not only is the human brain sparse in synaptic connectivity; it is also sparse in neuron activation rates. The energy consumed by a biological neuron is roughly proportional to the number of times it fires [ref]. So the fewer neurons that fire in the brain, the less energy it consumes. By contrast, a simulated neuron, like the one we described above, consumes the same amount of energy regardless of whether it fires or not. And if its output is zero, then that zero is still multiplied against the weights of the neurons that are wired to it.</p>

<p>In summary, the biological neural networks are sparse in synaptic connections and sparse in neuronal activations: they exhibit a ‚Äúdouble sparsity‚Äù which makes them very computationally efficient.</p>

<p>[maybe diagram]</p>

<p>Neural network researchers were aware of this fact and, throughout the 1990‚Äôs, spent great effort looking for different ways to make their models more sparse. One approach they used was ‚Äúweight magnitude regularization‚Äù which encouraged synaptic connections to go to zero if they weren‚Äôt improving the model‚Äôs accuracy [ref]. Another approach was ‚Äúmagnitude pruning‚Äù by which synaptic connections were removed entirely if their magnitudes were small [ref]. A related approach, cheerily named ‚ÄúOptimal Brain Damage‚Äù set synapses to zero in proportion to the sizes of their second derivatives (with respect to the model‚Äôs predictions) [ref]. Apart from direct pruning, they spent a great deal of time experimenting with different neural network architectures, like convolutional neural networks, where the number of synaptic connections didn‚Äôt grow quadratically with the number of neurons. [perhaps add some narrative an analysis here]</p>

<p>We should note that biological analogies were not the only reason that the researchers of the 1990‚Äôs cared about sparsity; they were also working under significant computational constraints. They were training their models on CPUs at the time, and CPUs back then were slow. Neural network research was a niche field that attracted limited funding [give stats, ref]. Most labs were academic and couldn‚Äôt afford large computer systems. And so in consequence, one of the best ways to train models with more neurons was to limit the total number of synapses (as the weights of the synapses were what occupied most of the memory).</p>

<p>So it was that from the early days, neural network researchers were passionately interested in incorporating sparsity into their models.</p>

<h2 id="how-gpus-and-automatic-differentiation-changed-things-better-title">How GPUs and Automatic Differentiation Changed Things [better title]</h2>

<p>In what is now famously known as the AI Winter, neural network research stagnated in the early 2000‚Äôs. Funding dropped precipitously and all but the most committed researchers left the field. But even as this happened, the tools and infrastructure that the field needed saw rapid growth. First and most importantly, the internet exploded into popular culture. As this happened, developers rushed to build the necessary tools and compute infrastructure required to support it at scale. Huge amounts of public domain image and text data became available. Tech companies built gigantic datacenters. Low-level programming languages became more efficient and high-level programming languages became more user-friendly (like Python).</p>

<p>The AI Winter came to an end in 2012 with the publication of AlexNet and the revelation that neural networks could outperform all existing methods of image classification by a large margin. And when this happened, all the tools ‚Äì better hardware in the form of GPUs, vast quantities of data in the public domain, and a growing cloud computing infrastructure built by tech companies ‚Äì were in place for a dramatic flowering of AI research.</p>

<p>(how sparsity was sidelined. what progress was made in sparsity. perhaps reference parts of this Numenta article)</p>

<h2 id="sparsity-in-modern-models">Sparsity in modern models</h2>

<h2 id="frontiers-of-sparsity">Frontiers of sparsity</h2>
:ET