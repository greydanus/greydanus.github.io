I"N,<p>Let’s imagine we were going to build a simulation of the universe. Starting from what we know about physics, how would we inmplement it? I wrote down a few ideas. It would be:</p>

<ol>
  <li><strong>Massively parallel.</strong> By taking advantage of the fact that all interactions are local and causal interactions cannot propagate faster than the speed of light, one could parallelize the simulation in a dramatic way. Spatially adjacent parts of the simulation would be run on the same CPUs and spatially distant parts would almost certainly be run on separate CPUs.</li>
  <li><strong>Conservation laws enforced.</strong> Physics is built on the idea that certain quantities are strictly conserved. Scalar quantities like mass-energy are conserved. So are vector quantities like spin and angular momentum.</li>
  <li><strong>Binary logic.</strong> Our computers use discrete, binary logic to represent and manipulate information. In this world, non-discrete numbers (eg. floats) are represented with sequences of discrete symbols. Let’s assume that a simulation of the universe would do the same thing.</li>
  <li><strong>Adaptive computation.</strong> A lot more happens in some times and places in the universe than others. To simulate it efficiently, we would not want to make a 3D grid and then put mass in each of the cells of that grid – 99% of the grid would be empty. Instead, we’d want to use a particle-based (Lagrangian) simulation where the distribution of matter and energy is represented as nodes on a graph.</li>
</ol>

<p>We can check to see whether these are good assumptions by looking at whether they hold true for existing state-of-the-art physics simulations. Taking a quick glance over the best <a href="https://www.myroms.org/">oceanography</a>, <a href="https://confluence.ecmwf.int/display/S2S/ECMWF+model+description">meteorology</a>, <a href="https://arxiv.org/abs/0810.5757">plasma</a>, and <a href="https://en.wikipedia.org/wiki/Computational_fluid_dynamics">computational fluid dynamics</a> models, this appear to be the case.</p>

<p>Now, having laid out some basic assumptions about our simulation of the universe, let’s look at their implications.</p>

<p>The first thing to see is that assumptions 1 and 2 are in tension with one another. In order to ensure that a quantity (eg. mass or energy or mass-energy) is conserved, you need to sum that quantity across the entire simulation, determine whether a correction is needed, and then apply that correction to the system as a whole. Computationally, this requires a synchronous reduction operation and an element-wise divide across the whole system, and this has to be done at virtually every simulation step. When you write a single-threaded physics simulation, this can account for about half of the computational cost (here are examples from the simple <a href="https://github.com/greydanus/optimize_wing/blob/3d661cae6ca6a320981fd5fc29848e1233d891cd/simulate.py#L57">fluid</a> and <a href="https://github.com/google-research/neural-structural-optimization/blob/1c11b8c6ef50274802a84cf1a244735c3ed9394d/neural_structural_optimization/topo_physics.py#L236">topology</a> simulations I have written).</p>

<p>As you parallelize the simulation, you can expect the cost of enforcing conservation laws to grow much higher. This is because simulating interactions between adjacent particles is pretty easy to do in a parallel and asynchronous manner. But enforcing system-wide conservation laws requires transferring data between distant CPU cores and keeping them more or less in sync. As a result, enforcing conservation laws in this manner quickly grows to be a limiting factor on runtime.</p>

<p>We may ask if perhaps there is some other way to enforce conservation laws without tallying conserved quantities across the entire simulation at each step. Probably yes! One way to do this is to quantize the conserved quantity. For example, we could quantize energy and then only transfer it in the form of discrete packets. Let’s take financial markets as an analogy. They are massively parallel and it’s important to keep a proper accounting of the total amount of currency in circulation at a given moment. The solution is to allow currency to function as a continuous value on a practical level, but to quantize it by making small measures of value (pennies) indivisible.</p>

<p>Quantization may work well for conserving scalar values like energy. But what about conserving vector quantities like angular momentum? In these cases, rotational symmetry makes things difficult. Imagine two particles with angular momentum vectors pointing in arbitrary directions. When we compute the angular momentum of the entire system, odds are we will get an irrational number. There is simply no way to quantize the directions of the two particles’ angular momentum vectors so that the sum of the two is also quantized without sacrificing rotational symmetry.</p>

<p>So how are we to implement exact conservation of vector quantities without sacrificing rotational symmetry? One option is to require that a particle’s vector quantities always be defined win reference to some other particle’s vector quantities. This would be implemented by creating multiple <a href="https://en.wikipedia.org/wiki/Pointer_(computer_programming)">pointer references</a> to a single variable and then giving each of those pointers to a different particle. As a concrete example, we might imagine an atom releasing energy in the form of two photons. The polarization angle of the first photon would be expressed as a pointer to variable <code class="language-plaintext highlighter-rouge">x</code> whereas that of the second photon would be expressed as a 90 degree rotation of a pointer to variable <code class="language-plaintext highlighter-rouge">x</code>. Moving our simulation forward, the polarization angles of the two phorons will change over time and perhaps some small amount of integration error will accumualte. But even if some integration error occurs, we can say with confidence that the net polarization of the two photons is zero because they are defined in terms of the same latent variable.</p>

<p>This approach is a good way to enforce exact conservation of vector quantities. However, it comes at a price. We must sacrifice locality – the principle that an object is influenced directly only by its immediate surroundings. It’s one of the most sacred principles in physics. This gets violated in the example of the two photons because a change in the polarization of the first photon will update the value of <code class="language-plaintext highlighter-rouge">x</code>, which will implicitly change the polarization of the second photon.</p>

<p>Interestingly, the mechanics of this nonlocal relationship would predict the violation of <a href="https://www.youtube.com/watch?v=zcqZHYo7ONs&amp;vl=en">Bell’s inequality</a>.</p>

<p>Physicists agree violation of Bell’s inequality implies that nature violates either realism (the principle that reality exists with definite properties even when not being observed) or locality. Since locality is seen as a more fundamental principle than realism, the modern consensus is that quantum mechanics violates realism: in other words, entangled particles cannot be said to have deterministic states and exist in a state of superposition until their are measured. As a contrast, in our simulated universe, realism would be preserved whereas exact locality would be sacrificed. Entangled particles could be said to have a definite state at all times, but whenever one particle interacted with some piece of matter, that interaction would affect the state of the other particle instantaneously.</p>

<p>This observation may lead us to ask whether other quantum mechanical phenomena are byproducts of simulation. The double slit experiment is an interesting example. We cannot explain a photon’s “self-interaction” using the argument we made above about conservation of vector quantities. Why, then, might particles exhibit wave-particle duality in a simulation?</p>

<p>The probable answer is filtering. Filtering is a common technique where a Gaussian (or cone) filter is convolved with a grid in order to smooth out the physics on the grid and make them appear as though they were occuring on a continuous medium. This step is absolutely essential when one wants to simulate continous physics on a discrete domain. The filtering steps in the <a href="https://github.com/greydanus/optimize_wing/blob/3d661cae6ca6a320981fd5fc29848e1233d891cd/simulate.py#L83">fluid</a> and <a href="https://github.com/google-research/neural-structural-optimization/blob/1c11b8c6ef50274802a84cf1a244735c3ed9394d/neural_structural_optimization/topo_physics.py#L84">topology</a> simulations I referred to earlier were just as essential as enforcing conservation laws. Moreover, filtering is used in most large-scale simulations like the ones we discussed at the beginning of the post.</p>

<p>How would one implement filtering in a large-scale particle-based simulation of the universe? Well, since the simulation is adaptively integrated across space, it would be hard to apply a Gaussian or cone filter directly. An alternative would be to simulate the dynamics of every particle as an ensemble. One would initialize a cloud of particles with slightly different initial conditions (but sharing pointer references to the same conserved quantities as discussed earlier) and then simulate all of them through time. If you allowed them to interact with one another, then collectively they would behave as a wave.</p>

<p>But you might notice that there is a tension between this spatially delocalized, wave-like behavior (a consequence of filtering, which is related to assumption 3) and the exact conservation of quantities like energy (assumption 2). The tension is this: when a wave interacts with an object, it transfers energy in a delocalized manner, in proportion to its amplitude. But we have decided to quantize energy in order to keep an exact accounting of it across our simulation. So when our ensemble of particles interacts with some matter, it must transfer exactly one quanta of energy and it must do so at one particular location. The simplest way to implement this would be to choose one particle out of the ensemble and allow it to transfer energy to other particles. The rest of the particles in the ensemble would be removed from the simulation upon coming into direct contact with some piece of matter.</p>

<p>To review – we’d need to implement filtering in our simulation in order to simulate continuous physics on a discrete domain. The best way to do it would be an ensemble method. And that ensemble method, along with the requirement that energy be quantized, would produce the empirical results we observe in the double slit experiment (namely: photons passing through both slits as if they were delocalized waves, but then transferring discrete quanta of energy to discrete locations on a photographic film).</p>

<p>Suppose we are living in a simulation, and the logistics of its implementation has led to some quantum mechanical effects. Suppose that in order to explain quantum mechanics we need to abandon locality assumptions and resurrect realism and determinism.</p>
:ET