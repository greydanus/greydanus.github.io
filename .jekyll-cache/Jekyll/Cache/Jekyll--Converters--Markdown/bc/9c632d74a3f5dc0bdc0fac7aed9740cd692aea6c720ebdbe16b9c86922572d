I"’1<p>The goal of Dissipative HNNs is to learn Hamiltonians from data even when the total energy of the system is not perfectly conserved (due to, for example, friction).</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hero.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  Dissipative HNNs (D-HNNs) output two scalar functions, denoted here by <i><b>H</b></i> and <i><b>D</b></i>. The first of these two, <i><b>H</b></i>, is the Hamiltonian. We use its symplectic gradient to model energy-conserving dynamics. The second, <i><b>D</b></i>, is the Rayleigh dissipation function. It models the dissipative component of the dynamics of a physical system. The addition of the dissipation function is what sets this model apart from Hamiltonian Neural Networks; it allows D-HNNs to model systems where energy is not quite conserved â€“ as, for example, in the case of a damped mass-spring system.
  </div>
</div>

<div style="display: block; margin-left: auto; margin-right:auto; width:100%; text-align:center;">
  <a href="https://arxiv.org/abs/2201.10085" id="linkbutton" target="_blank">Read the paper</a>
  <a href="https://github.com/greydanus/dissipative_hnns" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="a-sea-of-change">A sea of change</h2>

<p>We are immersed in a complex, dynamic world where change is the only constant. And yet there are certain patterns to this change that suggest natural laws. These laws include conservation of mass, energy, and momentum. Taken together, they constitute a powerful simplifying constraint on reality. Indeed, physics tells us that a small set of laws and their associated invariances are at the heart of all natural phenomena. Whether we are studying weather, ocean currents, earthquakes, or molecular interactions, we should take care to respect these laws. And when we apply learning algorithms to these domains, we should ensure that they, too, respect these laws.</p>

<p>We can do this by building models that are primed to learn invariant quantities from data: these models include <a href="https://greydanus.github.io/2019/05/15/hamiltonian-nns/">HNNs</a>, <a href="https://greydanus.github.io/2020/03/10/lagrangian-nns/">LNNs</a>, and a <a href="(https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C38&amp;q=hamiltonian+neural+networks&amp;btnG=)">growing class</a> of <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C38&amp;q=sympletic+neural+networks&amp;btnG=">related models</a>. But one problem with these models is that, for the most part, they can only handle data where some quantity (such as energy) is exactly conserved. If the data is collected in the real world and there is even a small amount of friction, then these models struggle. In this post, we introduce Dissipative HNNs, a class of models which can learn conservation laws from data even when energy isnâ€™t perfectly conserved.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/sea_of_change.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  We live in a sea of change. But no matter how complex a system's dynamics are, they can always be decomposed into the sum of dissipative dynamics and conservative dynamics.
  </div>
</div>

<p>The core idea is to use a neural network to parameterize both a Hamiltonian <em>and</em> a Rayleigh dissipation function. During training, the Hamiltonian function fits the conservative (rotational) component of the dynamics whereas the Rayleigh function fits the dissipative (irrotational) component. Letâ€™s dive in to how this works.</p>

<h2 id="a-quick-theory-section">A quick theory section</h2>

<p><strong>The Hamiltonian function.</strong> The Hamiltonian \(\mathcal{H}(\textbf{q},\textbf{p})\) is scalar function where by definition \( \frac{\partial \mathcal{H}}{\partial \textbf{p}} = \frac{\partial \textbf{q}}{dt},  -\frac{\partial \mathcal{H}}{\partial \textbf{q}} = \frac{\partial \textbf{p}}{dt} \). This constraint tells us that, even as the position and momentum coordinates of the system \(\textbf{(q, p)}\) change, the scalar output \(\mathcal{H}\) remains fixed. In other words, \(\mathcal{H}\) is invariant with respect to \(\textbf{q}\) and \(\textbf{p}\) as they change over time; it is a conserved quantity. Hamiltonians often appear in physics because for every natural symmetry (or law) in the universe, there is a corresponding conserved quantity (see <a href="https://en.wikipedia.org/wiki/Noether%27s_theorem">Noetherâ€™s theorem</a>).</p>

<p><strong>The Rayleigh function.</strong> The Rayleigh dissipation function \(\mathcal{D}(\textbf{q},\textbf{p})\) is a scalar function that provides a way to account for dissipative forces such as friction in the context of Hamiltonian mechanics. As an example, the Rayleigh function for linear, velocity-dependent dissipation would be \(\mathcal{D} = \frac{1}{2}\rho\dot{q}^2\) where \(\rho\) is a constant and \(\dot q\) is the velocity coordinate. We add this function to a Hamiltonian whenever the conserved quantity we are trying to model is changing due to sources and sinks. For example, if \(\mathcal{H}\) measures the total energy of a damped mass-spring system, then we could add the \(\mathcal{D}\) we wrote down above to account for the change in total energy due to friction.</p>

<p><strong>Helmholtz decompositions.</strong> Like many students today, Hermann von Helmholtz realized that medicine was not his true calling. Luckily for us, he switched to physics and discovered one of the most useful tools in vector analysis: the Helmholtz decomposition. The Helmholtz decomposition says that any vector field \(V\) can be written as the gradient of a scalar potential \(\phi\) plus the curl of a vector potential \(\mathcal{\textbf{A}}\). In other words, \( V = \nabla\phi + \nabla\times \mathcal{\textbf{A}}\). Note that the first term is irrotational and the second term is rotational. This tells us that <em>any vector field can be decomposed into the sum of an irrotational (dissipative) vector field and a rotational (conservative) vector field</em>. Hereâ€™s a visual example:</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hhd.jpg" />
</div>

<p><strong>Putting it together.</strong> In <a href="https://greydanus.github.io/2019/05/15/hamiltonian-nns/">Hamiltonian Neural Networks</a>, we showed how to parameterize the Hamiltonian function and then learn it directly from data. Here, we parameterize a Rayleigh function as well. Our model looks the same as an HNN except now it has a second scalar output which we use for the Rayleigh function (see the first image in this post). During the forward pass, we take the symplectic gradient of the Hamiltonian to obtain conservative forces. Note that as we do this, the symplectic gradient constitutes a rotational vector field over the modelâ€™s inputs. During the forward pass we also take the gradient of the Rayleigh function to obtain dissipative forces. This gradient gives us an irrotational vector field over the same domain. Thus, by construction, our model produces an implicit Helmholtz decomposition of the time derivatives of its inputs.</p>

<h2 id="an-introductory-model">An introductory model</h2>

<p>We coded up a D-HNN model and used it to fit three physical systems: a synthetic damped mass-spring, a real-world pendulum, and an ocean current timeseries sampled from the OSCAR dataset. In this post, weâ€™ll focus on the damped mass-spring example in order to build intuition for how D-HNNs work.</p>

<p>We can describe the state of a damped (one dimensional) mass-spring system with just two coordinates, \(q\) and \(p\). Also, we can plot these coordinates on Cartesian \(x\) and \(y\) axes to obtain <a href="https://en.wikipedia.org/wiki/Phase_space">phase-space diagrams</a>. These diagrams are useful because they allow us to visualize and compare our model to other baseline models.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/dampedspring.jpg" style="width:100%; max-width: 600px" />
</div>

<p>In the image above, the damped mass-spring dataset is plotted in the upper left square. Each arrow represents the time derivative of the system with respect to that \((p,q)\) coordinate. The Helmholtz decomposition tells us that this vector field can be decomposed into conservative and dissipative components, and indeed that is what we have done in the second and third columns.<sup id="fnref:fn1" role="doc-noteref"><a href="#fn:fn1" class="footnote">1</a></sup> You may notice that the dissipative field in the third column isolates the force due to friction.</p>

<p>In the second row, we evaluate a D-HNN trained on the system. The D-HNN produces a trajectory that closely matches ground truth. By plotting the symplectic gradient of \(\mathcal{H}\) and the gradient of \(\mathcal{D}\), we can see that it has properly decoupled the conservative and dissipative dynamics respectively. By contrast, in the third row, we train a baseline model (an MLP) on the same data; this model produces a good trajectory but is unable to learn conservative and dissipative dynamics separately. Finally, in the fourth row, we train an HNN on the same dataset and find that it is only able to model the conservative component of the systemâ€™s dynamics. It strictly enforces conservation of energy in a scenario where energy is not actually conserved, leading to a poor prediction.</p>

<h2 id="why-decouple-conservative-and-dissipative-dynamics">Why decouple conservative and dissipative dynamics?</h2>

<p>Weâ€™ve described a model that can learn conservative and dissipative dynamics separately and shown that it works on a toy problem. Why is this a good idea? One answer is that <em>it lets our model fit data in a more physically-realistic manner, leading to better generalization</em>.</p>

<p>If we were to suddenly double the coefficient of friction \(\rho\), our MLP model would not be able to predict a viable trajectory. This is because it models the dissipative and conservative dynamics of the system together. However, since our D-HNN learned these dynamics separately, <em>it can generalize to new friction coefficients without additional training</em>. In order to double the scale of dissipative forces, we can simply multiply the gradient of the Rayleigh function by two. The image below shows how this produces viable trajectories under unseen friction coefficients (orange highlights).</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/spring_rho.jpg" style="width:100%; max-width: 600px" />
</div>

<h2 id="additional-experiments">Additional experiments</h2>

<p>We also trained our model on data from a real pendulum and ocean current data from the OSCAR dataset (shown below). On these larger and more more difficult tasks, our model continuted to decouple conservative and dissipative dynamics. The details and results are outside the scope of this post. If you want to see them, you should read <a href="https://github.com/greydanus/dissipative_hnns">the full paper</a>.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/ocean.jpg" style="width:100%" />
</div>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>We see D-HNNs as a small, practical contribution because they can help us analyze our datasets in a new, more physically-accurate, manner. They also contribute towards a more ambitious goal: the goal of building machine learning odels that can make sense of the incredible complexity of the real world by focusing on its invariant quantities.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:fn1" role="doc-endnote">
      <p>note: in practice, we performed this decomposition using a few hundred iterations of the Gauss-Seidel method to solve Poissonâ€™s equation. Again, see <a href="https://drive.google.com/file/d/1upKFdtnM0xcTVxNsPHI1KCvmcanAJheL/view?usp=sharing">this paper</a> for details.Â <a href="#fnref:fn1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET