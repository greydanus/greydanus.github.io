I"Y%<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hero.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  Dissipative HNNs (D-HNNs) output two scalar functions, denoted here by <i><b>H</b></i> and <i><b>D</b></i>. The first of these two, <i><b>H</b></i>, is the Hamiltonian. We use its symplectic gradient to model energy-conserving dynamics. The second, <i><b>D</b></i>, is the Rayleigh dissipation function. It models the dissipative component of the dynamics of a physical system. The addition of the dissipation function is what sets this model apart from Hamiltonian Neural Networks; it allows D-HNNs to model systems where energy is not quite conserved – as, for example, in the case of a damped mass-spring system.
  </div>
</div>

<div style="display: block; margin-left: auto; margin-right:auto; width:100%; text-align:center;">
  <a href="https://arxiv.org/abs/2201.10085" id="linkbutton" target="_blank">Read the paper</a>
  <a href="https://github.com/greydanus/dissipative_hnns" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="a-sea-of-change">A sea of change</h2>

<p>We are immersed in a complex, dynamic world where change is the only constant. And yet there are certain patterns to this change that suggest natural laws. These laws include conservation of mass, energy, and momentum. Taken together, they constitute a powerful simplifying constraint on reality. Indeed, physics tells us that a small set of laws and their associated invariances are at the heart of all natural phenomena. Whether we are studying weather, ocean currents, earthquakes, or molecular interactions, we should take care to respect these laws. And when we apply learning algorithms to these domains, we should ensure that they, too, respect these laws.</p>

<p>We can do this by building models that are primed to learn invariant quantities from data: these models include HNNs, LNNs, and a growing class of related models. But one problem with this models is that, for the most part, they can only handle data where some quantity (such as energy) is exactly conserved. If the data is collected in the real world, and there is even a small amount of friction, then these models will struggle. In this post, we’ll indroduce Dissipative HNNs, a class of models which can learn conservation laws from data even when energy isn’t perfectly conserved.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/sea_of_change.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  We live in a sea of change. But no matter how complex a system's dynamics are, they can always be decomposed into the sum of dissipative dynamics and conservative dynamics.
  </div>
</div>

<p>The core idea is to parameterize both a Hamiltonian <em>and</em> a Rayleigh dissipation function. During training, the Hamiltonian function fits the conservative (rotational) component of the dynamics whereas the Rayleigh function fits the dissipative (irrotational) component.</p>

<h2 id="a-quick-theory-section">A quick theory section</h2>

<p><strong>The Hamiltonian function.</strong> The Hamiltonian \(\mathcal{H}(\textbf{q},\textbf{p})\) is scalar function where by definition \( \frac{\partial \mathcal{H}}{\partial \textbf{p}} = \frac{\partial \textbf{q}}{dt},  -\frac{\partial \mathcal{H}}{\partial \textbf{q}} = \frac{\partial \textbf{p}}{dt} \). This constraint tells us that, even as the position and momentum coordinates of the system \(\textbf{(q, p)}\) change, the scalar output \(\mathcal{H}\) remains fixed. In other words, \(\mathcal{H}\) is invariant with respect to \(\textbf{q}\) and \(\textbf{p}\) as they change over time; it is a conserved quantity. Hamiltonians often appear often in physics because, for every natural symmetry (or law), there is a corresponding conserved quantity (see <a href="https://en.wikipedia.org/wiki/Noether%27s_theorem">Noether’s theorem</a>).</p>

<p><strong>The Rayleigh function.</strong> The Rayleigh dissipation function \(\mathcal{D}(\textbf{q},\textbf{p})\) is a scalar function that provides a way to account for dissipative forces such as friction in the context of Hamiltonian mechanics. As an example, the Rayleigh function for linear, velocity-dependent dissipation would be \(\mathcal{D} = \frac{1}{2}\rho\dot{q}^2\) where \(\rho\) is a constant and \(\dot q\) is the velocity coordinate. We add this function to a Hamiltonian whenever the conserved quantity we are trying to model is changing due to sources and sinks. For example, if \(\mathcal{H}\) measures the total energy of a damped mass-spring system, then we could add the \(\mathcal{D}\) we wrote down above to account for the change in total energy due to friction.</p>

<p><strong>Helmholtz decompositions.</strong> Like many students today, Hermann von Helmholtz realized that medicine was not his true calling. Luckily for us, he pursued physics instead and discovered one of the most useful tools in vector analysis: the Helmholtz decomposition. The Helmholtz decomposition says that any vector field \(V\) can be written as the gradient of a scalar potential \(\phi\) plus the curl of a vector potential \(\mathcal{\textbf{A}}\). In other words, \( V = \nabla\phi + \nabla\times \mathcal{\textbf{A}}\). Now, note that the first term is irrotational and the second term is rotational. This tells us is that any vector field can be decomposed into the sum of an irrotational (dissipative) vector field and a rotational (conservative) vector field. For example:</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hhd.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">This figure was taken and modified from <a href="https://drive.google.com/file/d/1upKFdtnM0xcTVxNsPHI1KCvmcanAJheL/view?usp=sharing">Stam, 2003</a>.
  </div>
</div>

<p>It based on the observation that any smooth vector field \(V\) can be expressed the sum of an irrotational vector field \(V_{irr}\) and a rotational vector field \(V_{rot}\).</p>

<p>\cite{helmholtz1858integrale}. More formally, $V$ can be expressed as the sum of the curl-less gradient of a scalar potential $\phi$ and the divergence-less curl of a vector potential $\mathcal{\textbf{A}}$.
\begin{equation}
    \( V = V_{irr} + V_{rot} = \nabla\phi + \nabla\times \mathcal{\textbf{A}}\)
\end{equation}
D-HNNs performing an implicit Helmholtz decomposition can be found in Figure \ref{fig:dampedspring} and Figure \ref{fig:dampedspring_counterfactual}.</p>

<p>Taken together, they represent an implicit Helmholtz decomposition.</p>

<h2 id="experiments">Experiments</h2>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/dampedspring.jpg" style="width:80%" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  Training a Dissipative Hamiltonian Neural Network (D-HNN) and several baseline models on a damped spring task. <b>Row 1.</b> We decompose the original dataset by interpolating the training data onto a grid with a nearest-neighbors approach and then decomposing the field into two components. In practice, we accomplish this using a few hundred iterations of the Gauss-Seidel method to solve Poisson's equation. <b>Row 2.</b> We train a D-HNN on the same data. The first half of this model looks exactly like an HNN. The second half also looks like an HNN, except we use the gradient of the scalar field directly rather than rearranging it according to Hamilton's equations. This gives us a trainable \textit{dissipative} field which is able to model the dissipative components of the damped spring's dynamics. Summing these two fields, we obtain an accurate model of the dynamics of the system. <b>Row 3.</b> We train the baseline model (MLP) on the training set; this model gives a good fit and can be integrated as a Neural ODE, but it cannot be used to decompose the field into conservative and dissipative components. <b>Row 4.</b> We train a Hamiltonian Neural Network (HNN) on the same dataset and find that it is only able to model the conservative component of the system's dynamics. In other words, it strictly enforces conservation of energy in a scenario where energy is not actually conserved.
  </div>
</div>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>D-HNNs build on the foundations of HNNs by bridging the gap between the clean, natural symmetries of physics and the imperfect, messy data often found in the real world. They employ the tools of Hamiltonian mechanics and Helmholtz decomposition to separate conserved quantities from dissipative quantities. In doing so, they allow us to model complex physical systems while enforcing strict conservation laws. They represent a small, practical advance in that they can help us see our datasets, like the ocean currents dataset, in a new way. And at the same time, they represent progress towards the more ambitious goal of building models which make sense of the incredible complexity of the real world by focusing on its invariant quantities.</p>

<h2 id="footnotes">Footnotes</h2>

:ET