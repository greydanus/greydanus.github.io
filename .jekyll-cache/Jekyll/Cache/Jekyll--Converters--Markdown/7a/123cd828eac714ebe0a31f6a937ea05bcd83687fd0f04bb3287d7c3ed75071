I")3<p>The goal of Dissipative HNNs is to learn Hamiltonians from data even when the total energy of the system is not perfectly conserved (due to, for example, friction).</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hero.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  Dissipative HNNs (D-HNNs) output two scalar functions, denoted here by <i><b>H</b></i> and <i><b>D</b></i>. The first of these two, <i><b>H</b></i>, is the Hamiltonian. We use its symplectic gradient to model energy-conserving dynamics. The second, <i><b>D</b></i>, is the Rayleigh dissipation function. It models the dissipative component of the dynamics of a physical system. The addition of the dissipation function is what sets this model apart from Hamiltonian Neural Networks; it allows D-HNNs to model systems where energy is not quite conserved – as, for example, in the case of a damped mass-spring system.
  </div>
</div>

<div style="display: block; margin-left: auto; margin-right:auto; width:100%; text-align:center;">
  <a href="https://arxiv.org/abs/2201.10085" id="linkbutton" target="_blank">Read the paper</a>
  <a href="https://github.com/greydanus/dissipative_hnns" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="a-sea-of-change">A sea of change</h2>

<p>We are immersed in a complex, dynamic world where change is the only constant. And yet there are certain patterns to this change that suggest natural laws. These laws include conservation of mass, energy, and momentum. Taken together, they constitute a powerful simplifying constraint on reality. Indeed, physics tells us that a small set of laws and their associated invariances are at the heart of all natural phenomena. Whether we are studying weather, ocean currents, earthquakes, or molecular interactions, we should take care to respect these laws. And when we apply learning algorithms to these domains, we should ensure that they, too, respect these laws.</p>

<p>We can do this by building models that are primed to learn invariant quantities from data: these models include <a href="https://greydanus.github.io/2019/05/15/hamiltonian-nns/">HNNs</a>, <a href="https://greydanus.github.io/2020/03/10/lagrangian-nns/">LNNs</a>, and a <a href="(https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C38&amp;q=hamiltonian+neural+networks&amp;btnG=)">growing class</a> of <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C38&amp;q=sympletic+neural+networks&amp;btnG=">related models</a>. But one problem with these models is that, for the most part, they can only handle data where some quantity (such as energy) is exactly conserved. If the data is collected in the real world and there is even a small amount of friction, then these models struggle. In this post, we introduce Dissipative HNNs, a class of models which can learn conservation laws from data even when energy isn’t perfectly conserved.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/sea_of_change.jpg" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  We live in a sea of change. But no matter how complex a system's dynamics are, they can always be decomposed into the sum of dissipative dynamics and conservative dynamics.
  </div>
</div>

<p>The core idea is to use a neural network to parameterize both a Hamiltonian <em>and</em> a Rayleigh dissipation function. During training, the Hamiltonian function fits the conservative (rotational) component of the dynamics whereas the Rayleigh function fits the dissipative (irrotational) component. Let’s dive in to how this works.</p>

<h2 id="a-quick-theory-section">A quick theory section</h2>

<p><strong>The Hamiltonian function.</strong> The Hamiltonian \(\mathcal{H}(\textbf{q},\textbf{p})\) is scalar function where by definition \( \frac{\partial \mathcal{H}}{\partial \textbf{p}} = \frac{\partial \textbf{q}}{dt},  -\frac{\partial \mathcal{H}}{\partial \textbf{q}} = \frac{\partial \textbf{p}}{dt} \). This constraint tells us that, even as the position and momentum coordinates of the system \(\textbf{(q, p)}\) change, the scalar output \(\mathcal{H}\) remains fixed. In other words, \(\mathcal{H}\) is invariant with respect to \(\textbf{q}\) and \(\textbf{p}\) as they change over time; it is a conserved quantity. Hamiltonians often appear in physics because for every natural symmetry (or law) in the universe, there is a corresponding conserved quantity (see <a href="https://en.wikipedia.org/wiki/Noether%27s_theorem">Noether’s theorem</a>).</p>

<p><strong>The Rayleigh function.</strong> The Rayleigh dissipation function \(\mathcal{D}(\textbf{q},\textbf{p})\) is a scalar function that provides a way to account for dissipative forces such as friction in the context of Hamiltonian mechanics. As an example, the Rayleigh function for linear, velocity-dependent dissipation would be \(\mathcal{D} = \frac{1}{2}\rho\dot{q}^2\) where \(\rho\) is a constant and \(\dot q\) is the velocity coordinate. We add this function to a Hamiltonian whenever the conserved quantity we are trying to model is changing due to sources and sinks. For example, if \(\mathcal{H}\) measures the total energy of a damped mass-spring system, then we could add the \(\mathcal{D}\) we wrote down above to account for the change in total energy due to friction.</p>

<p><strong>Helmholtz decompositions.</strong> Like many students today, Hermann von Helmholtz realized that medicine was not his true calling. Luckily for us, he switched to physics and discovered one of the most useful tools in vector analysis: the Helmholtz decomposition. The Helmholtz decomposition says that any vector field \(V\) can be written as the gradient of a scalar potential \(\phi\) plus the curl of a vector potential \(\mathcal{\textbf{A}}\). In other words, \( V = \nabla\phi + \nabla\times \mathcal{\textbf{A}}\). Note that the first term is irrotational and the second term is rotational. This tells us that <em>any vector field can be decomposed into the sum of an irrotational (dissipative) vector field and a rotational (conservative) vector field</em>. Here’s a visual example:</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hhd.jpg" />
</div>

<p><strong>Putting it together.</strong> In <a href="https://greydanus.github.io/2019/05/15/hamiltonian-nns/">Hamiltonian Neural Networks</a>, we showed how to parameterize the Hamiltonian function and then learn it directly from data. Here, we parameterize a Rayleigh function as well. Our model looks the same as an HNN except now it has a second scalar output which we use for the Rayleigh function (see the first image in this post). During the forward pass, we take the symplectic gradient of the Hamiltonian to obtain conservative forces. Note that as we do this, the symplectic gradient constitutes a rotational vector field over the model’s inputs. During the forward pass we also take the gradient of the Rayleigh function to obtain dissipative forces. This gradient gives us an irrotational vector field over the same domain. Thus, by construction, our model produces an implicit Helmholtz decomposition of the time derivatives of its inputs.</p>

<h2 id="an-introductory-model">An introductory model</h2>

<p>We coded up a D-HNN model and used it to fit three physical systems: a synthetic damped mass-spring, a real-world pendulum, and an ocean current timeseries sampled from the OSCAR dataset. In this post, we’ll focus on the damped mass-spring example in order to build intuition for how D-HNNs work.</p>

<p>We can describe a 1D damped mass-spring system with just two coordinates, \(q\) and \(p\). We can plot them on \(x\) and \(y\) axes respectively to obtain <a href="https://en.wikipedia.org/wiki/Phase_space">phase-space diagrams</a>. These diagrams are useful because, once we fit our model and a few baselines to this dataset, we plotted their predictions in phase space in order to compare them to each other, and to a ground-truth trajectory.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/dampedspring.jpg" style="width:100%; max-width: 600px" />
</div>

<p>In the image above, you can find the entire damped mass-spring dataset plotted in the upper left square. Each arrow is located at a \((p,q)\) coordinate which is also the input to our model. The direction and magnitude of the arrow represents the corresponding time derivative of those coordinates. Obviously, this plot is a vector field. The Helmholtz decomposition tells us that this vector field can be decomposed into conservative and dissipative components, and indeed that is what we have done in the second and third columns (note: in practice, we performed this decomposition using a few hundred iterations of the Gauss-Seidel method to solve Poisson’s equation. Again, see <a href="https://drive.google.com/file/d/1upKFdtnM0xcTVxNsPHI1KCvmcanAJheL/view?usp=sharing">this paper</a> for details.)</p>

<p>Now, in the second row, we train a D-HNN on this dataset. It produces a trajectory that closely matches the ground truth. By plotting the symplectic gradient of \(\mathcal{H}\) and the gradient of \(\mathcal{D}\), we can see that it has properly decoupled the conservative and dissipative dynamics respectively. By contrast, in the third row, we train a baseline model (an MLP) on the same data; this model produces a good trajectory but is unable to learn conservative and dissipative dynamics separately (the second and third columns are thus left blank). Finally, in the fourth row, we train an HNN on the same dataset and find that it is only able to model the conservative component of the system’s dynamics. It strictly enforces conservation of energy in a scenario where energy is not actually conserved, leading to a poor trajectory prediction.</p>

<h2 id="why-decouple-conservative-and-dissipative-dynamics">Why decouple conservative and dissipative dynamics?</h2>

<p>We’ve described a model that can learn conservative and dissipative dynamics separately and shown that it works on a toy problem. Why is this a good idea? One answer is that <em>it lets our model fit data in a more physically-realistic manner, leading to better generalization</em>.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/spring_rho.jpg" style="width:100%; max-width: 600px" />
</div>

<p>In the toy example above, we our dataset was constructed using a single friction coefficient, \(\rho\). The MLP we trained gives good predictions for this friction coefficient, but it would be useless if we wanted to predict the mass’s trajectory if friction were halved. However, since our D-HNN models friction as a dissipative force, we can do exactly that by multiplying its dissipative vector field by a factor of two or one half. The image above shows that it can produce physically plausible trajectories for these new friction coefficients (highlighted in orange) <em>even though it was only trained on data with one friction coefficient</em>.</p>

<h2 id="additional-experiments">Additional experiments</h2>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/ocean.jpg" style="width:100%" />
</div>

<p>We also trained our model on data from a real pendulum and ocean current data from the OSCAR dataset (a visualization of our model’s fit on the ocean dataset is shown above). On these larger and more more difficult tasks, our model continuted to decouple conservative and dissipative dynamics. The details and results are outside the scope of this post – if you want to see them, you should read <a href="https://github.com/greydanus/dissipative_hnns">the full paper</a>.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>We see D-HNNs as a small, practical contribution because they can help us analyze our datasets in a new, more physically-accurate, manner. They also contribute towards a more ambitious goal: the goal of building machine learning odels that can make sense of the incredible complexity of the real world by focusing on its invariant quantities.</p>

<!-- ## Footnotes

[^fn0]: A largely subjective observation was that Jumpy RNNs seemed easier to train and scale than Neural ODEs. With that said, one should note that Neural ODEs are improving at a rapid pace, and so this may change as time passes.
[^fn1]: Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. [A Clockwork RNN](https://arxiv.org/abs/1402.3511). _International Conference on Machine Learning_, pp. 1863–1871, 2014. -->

:ET