I"<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%">
  <img src="/assets/dissipative-hnns/hero.png" />
  <div class="thecap" style="text-align:left; display:block; margin-left: auto; margin-right: auto; width:100%">
  Dissipative HNNs (D-HNNs) output two scalar functions, denoted here with <b>H</b> and <b>D</b>. The first of these two, <b>H</b>, is the Hamiltonian. It is perfectly conserved. The second of these two, <b>D</b>, is the Rayleigh dissipation function. It models the dissipative component of the dynamics of a physical system. The addition of the dissipation function is what sets this model apart from Hamiltonian Neural Networks; it allows D-HNNs to model systems where energy is not quite conserved, as, for example, in the case of a damped mass-spring system.
  </div>
</div>

<div style="display: block; margin-left: auto; margin-right:auto; width:100%; text-align:center;">
  <a href="https://arxiv.org/abs/2201.10085" id="linkbutton" target="_blank">Read the paper</a>
  <a href="https://github.com/greydanus/hamiltonian-nn" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="the-wisdom-of-learning-invariant-quantities">The Wisdom of Learning Invariant Quantities</h2>

<p>We are immersed in a complex, dynamic world where change is the only constant. And yet there are certain patterns to this change that suggest natural laws. These laws include conservation of mass, energy, and momentum. Taken together, they constitute a powerful simplifying constraint on reality. Indeed, physics tells us that a small set of laws and their associated invariances are at the heart of all natural phenomena. Whether we are studying weather, ocean currents, earthquakes, or molecular interactions, we should take care to respect these laws. And when we apply learning algorithms to these domains, we should ensure that they, too, take these laws into account.</p>

<p>One of the remarkable insights of physics is that all of these laws can be expressed as conserved quantities. For example, conservation of angular momentum enforces rotational invariance and conservation of mass-energy enforces time-translation invariance \cite{noether1971invariant}. The mathematical framework for representing these conserved quantities is called Hamiltonian mechanics, and the function which relates the coordinates of a physical system to a conserved quantity (usually energy) is called the Hamiltonian. Recent work by \citet{greydanus2019hamiltonian}, \citet{toth2019hamiltonian} and others has shown that this function can be used to incorporate physical constraints, like conservation of energy, into deep neural networks.</p>

<p>These models, which we will refer to as Hamiltonian Neural Networks (HNNs), are able to learn more accurate representations of the world than would otherwise be possible. Recent works by \citet{cranmer2020lagrangian}, \citet{ chen2019symplectic}, and \citet{choudhary2020physics} have shown how to learn Hamiltonians for nonlinear, chaotic systems such as double pendulums, $n$-body systems, and bouncing balls. Moreover, these models can be learned starting from perceptually difficult observations, such as pixel videos. A number of other works have begun applying these physics-inspired models to new fields such as quantum mechanics \cite{broughton2020tensorflow}, chaos theory \cite{choudhary2020physics}, and differential equations \cite{mattheakis2020hamiltonian}.</p>

<p>One remaining drawback of HNNs is that they assume that the total energy of the observed system is conserved. In other words, they assume the absence of friction and other sources or sinks of energy. This obscures an important fact about the real world: no matter how sensitive one’s measurements are, real-world data never conforms exactly to theoretical expectations. Real-world dynamic systems often contain dissipative dynamics due to friction, external forces, or measurement noise. How might we enable HNNs to learn conserved quantities even in the presence of noisy real-world data?</p>

<p>In this paper, we tackle the problem of learning in real-world dynamic systems by introducing Dissipative Hamiltonian Neural Networks (D-HNNs), a model that extends HNNs to learn the decomposition of any dynamic system into its dissipative and conserved quantities. D-HNNs learn decompositions by learning a Rayleigh dissipation function $\mathcal{D}$ in addition to a Hamiltonian. The resulting network has two scalar outputs, $\mathcal{H}$ and $\mathcal{D}$. The first yields a rotational vector field with respect to the input coordinates and the second yields an irrotational vector field over the same coordinates. Summed together, the Hodge-Helmholtz theorem tells us that these two vector fields can fit any arbitrary vector field. By the same token, any vector field – say, a dataset of velocity measurements – can be decomposed into the sum of a rotational vector field and an irrotational vector field. We call this decomposition the Helmholtz decomposition \cite{bhatia2012helmholtz}.</p>

<p>When we train our model on a dataset, it learns an implicit Helmholtz decomposition of the dynamics. In other words, it learns the dissipative and conservative dynamics separately. When we train it on observations of a damped spring, for example, our model learns a Hamiltonian resembling that of a non-damped spring while using the Rayleigh dissipation function to model the effects of friction. Later in this paper, we show that by multiplying the Rayleigh function by various constants, we can estimate what dynamics would occur for different friction coefficients. Thus we can answer counterfactual questions about a system such as \textit{what would happen if the friction were halved?} On the spring task and others, we show that D-HNNs match or outperform baseline models and non-ML numerical approaches.</p>

<p>Learning decompositions of dynamic systems has practical applications for other fields of vector analysis such as computer graphics, fluid dynamics, weather modeling, and even forensics \cite{bhatia2012helmholtz}. One example we consider in this work is oceanography. Oceanographers are often interested in locating upwelling, a dissipative feature, which occurs when cold, nutrient-rich waters rise to the surface of the ocean, supporting large populations of fish and other marine life. Finding such dissipative patterns in ocean current data is a non-trivial task. In this work, we apply D-HNNs to a large ocean-current dataset and use them to map such features.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>Neural</p>

<h2 id="footnotes">Footnotes</h2>

:ET